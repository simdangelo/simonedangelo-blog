{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way we can imagine **Threading** is like adding a new lanes to a highway.\n",
    "\n",
    "If you have a single lane highway, then all cars have to follow each other and, if one car is going slow, then all the cars behind it have to go slow too. But, if you have a second or third lane, then the faster cars can pass the slower cars.\n",
    "\n",
    "Let's get into Threading with a toy example that prints names and ages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John\n",
      "Kate\n",
      "Mike\n",
      "Alex\n",
      "Ann\n",
      "26\n",
      "23\n",
      "41\n",
      "31\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "import random\n",
    "\n",
    "def print_names():\n",
    "    for name in (\"John\", \"Kate\", \"Mike\", \"Alex\", \"Ann\"):\n",
    "        print(name)\n",
    "        time.sleep(random.uniform(0.5, 1.5))\n",
    "\n",
    "def print_age():\n",
    "    for _ in range(5):\n",
    "        print(random.randint(20, 50))\n",
    "        time.sleep(random.uniform(0.5, 1.5))\n",
    "\n",
    "print_names()\n",
    "print_age()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the expected behaviour. Now, let's:\n",
    "* create two `Thread` objects\n",
    "* then call them do to some stuff\n",
    "* use `join()` on both threads. The way we can think about `join()` is that we're joining back up with the main thread: we split out the highway into multiple lanes and then we are bringing now all those multiple lines into a single lane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John\n",
      "47\n",
      "Kate\n",
      "26\n",
      "Mike\n",
      "24\n",
      "21\n",
      "Alex\n",
      "24\n",
      "Ann\n"
     ]
    }
   ],
   "source": [
    "t1 = threading.Thread(target=print_names)\n",
    "t2 = threading.Thread(target=print_age)\n",
    "\n",
    "t1.start()\n",
    "t2.start()\n",
    "\n",
    "t1.join()\n",
    "t2.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the execution is passing back and forth between the first thread and the second thread: basically when the thread 1 is sleeping, the execution gets passed over to thread 2 and viceversa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## passing arguments into a function\n",
    "\n",
    "If we want to pass some arguments to our function, we can use the `args` parameter in the `Thread()` class instantiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John\n",
      "28\n",
      "29\n",
      "Kate\n",
      "46\n",
      "46\n",
      "31\n",
      "Mike\n",
      "Alex\n",
      "Ann\n"
     ]
    }
   ],
   "source": [
    "def print_names():\n",
    "    for name in (\"John\", \"Kate\", \"Mike\", \"Alex\", \"Ann\"):\n",
    "        print(name)\n",
    "        time.sleep(random.uniform(0.5, 1.5))\n",
    "\n",
    "def print_age(min_sleep, max_sleep):\n",
    "    for _ in range(5):\n",
    "        print(random.randint(20, 50))\n",
    "        time.sleep(random.uniform(min_sleep, max_sleep))\n",
    "\n",
    "t1 = threading.Thread(target=print_names)\n",
    "t2 = threading.Thread(target=print_age, args=(0.2, 1))\n",
    "\n",
    "t1.start()\n",
    "t2.start()\n",
    "\n",
    "t1.join()\n",
    "t2.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the ages are running significantly faster than the names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## more common example \n",
    "\n",
    "Let's take a look now into a more common example that you would actually want to use threading for. But, firstly, let's talk about Threads in Python because it might be a bit different than in other languages.\n",
    "\n",
    "In Python, **Threads aren't truly parallel**. This is due to the **Global Interpreter Lock** (**GIL**). The GIL prevents multiple threads from executing Python by code at the same time. There are many reasons for this and there are pros and cons to the GIL, but the main reason is it makes much easier to write thread safe code. The main thing to keep in mind when deciding whether to use thrads or not is whether your task is **CPU-bound** or **I/O-bound**. CPU-bound tasks are tasked to require a lor of computation whereas for I/O bound tasks they do involve a good amount of waiting around (whether that is for user input, whether you're downloading files, whether you're reaching out to APIs, whether you're writing files, etc...). For those CPU-bound tasks, you are likely going to want to use **Multi-Processing** (*TODO: add link*), but for **I/O-bound tasks**, Threading is the way to go.\n",
    "\n",
    "Let's give an example where we just want to download some files from a website:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://raw.githubusercontent.com/JacobCallahan/Understanding/master/Python/file_io/binary_file\n",
      "threading_lesson_download_folder/binary_file\n",
      "Downloading https://raw.githubusercontent.com/JacobCallahan/Understanding/master/Python/file_io/binary_file to threading_lesson_download_folder/binary_file\n",
      "https://raw.githubusercontent.com/JacobCallahan/Understanding/master/Python/file_io/files.py\n",
      "threading_lesson_download_folder/files.py\n",
      "Downloading https://raw.githubusercontent.com/JacobCallahan/Understanding/master/Python/file_io/files.py to threading_lesson_download_folder/files.py\n",
      "https://raw.githubusercontent.com/JacobCallahan/Understanding/master/Python/file_io/names.txt\n",
      "threading_lesson_download_folder/names.txt\n",
      "Downloading https://raw.githubusercontent.com/JacobCallahan/Understanding/master/Python/file_io/names.txt to threading_lesson_download_folder/names.txt\n",
      "https://raw.githubusercontent.com/JacobCallahan/Understanding/master/Python/file_io/new_file.txt\n",
      "threading_lesson_download_folder/new_file.txt\n",
      "Downloading https://raw.githubusercontent.com/JacobCallahan/Understanding/master/Python/file_io/new_file.txt to threading_lesson_download_folder/new_file.txt\n",
      "Finished downloaind threading_lesson_download_folder/binary_file\n",
      "Finished downloaind threading_lesson_download_folder/names.txt\n",
      "Finished downloaind threading_lesson_download_folder/new_file.txt\n",
      "Finished downloaind threading_lesson_download_folder/files.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import threading\n",
    "import requests\n",
    "from  pathlib import Path\n",
    "\n",
    "def download_file(url, filename):\n",
    "    print(f\"Downloading {url} to {filename}\")\n",
    "    response = requests.get(url)\n",
    "    Path(filename).write_bytes(response.content)\n",
    "    print(f\"Finished downloaind {filename}\")\n",
    "\n",
    "base_url = \"https://raw.githubusercontent.com/JacobCallahan/Understanding/master/Python/file_io\"\n",
    "urls = [\n",
    "    f\"{base_url}/binary_file\",\n",
    "    f\"{base_url}/files.py\",\n",
    "    f\"{base_url}/names.txt\",\n",
    "    f\"{base_url}/new_file.txt\",\n",
    "]\n",
    "\n",
    "threads = []\n",
    "for url in urls:\n",
    "    print(url)\n",
    "    filename = \"threading_lesson_download_folder/\" + url.split(\"/\")[-1]\n",
    "    print(filename)\n",
    "    t = threading.Thread(target=download_file, args=(url, filename))\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "[t.join() for t in threads]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since some files took longer than others, we did get to see that Python pass back execution between the different threads, so the faster one (`binary_file`) finish before the slower ones.\n",
    "\n",
    "While you could technically stop there, this isn't the way I like to handle threads in my projects. In fact, I like to use something that was introduces midway through Python3 development and that is using sometinhf it came from `cuncurrent.future`. This module gives us a conventient way to deal with threads and it offers an interface that makes a few things a litle bit more simple. The first thing to notice is that we're actually given a chance to use a context manager because, as we said in the lesson about context manager (*TODO: add link*), it is useful to deal with the clean up so we don't have to do that manually.\n",
    "\n",
    "We'll use the `ThreadPoolExecutor` class where one of the most important argument is `max_workers`: remember that in the previous example we had four different URLs and we created a thread for each of those. The question is: did we really need four threads to download those four files? Actually it depends on your priorities, but that if you had 1000 files? Do you really want to spin up 1000 threads? For most people the answer is \"NO\", unless you have a way of manually balancing workloads between threads. `ThreadPoolExecutor` are fantastic interface to do that for you and you can specify the maximum amount of Threads (`max_workers`) you want to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting status of https://www.google.com\n",
      "Getting status of https://www.facebook.com\n",
      "Finished getting status of https://www.facebook.com in 0.21 seconds\n",
      "Getting status of https://www.twitter.com\n",
      "Status code for https://www.facebook.com is 200\n",
      "Finished getting status of https://www.google.com in 0.31 seconds\n",
      "Getting status of https://www.github.com\n",
      "Status code for https://www.google.com is 200\n",
      "Finished getting status of https://www.github.com in 0.29 seconds\n",
      "Getting status of https://www.linkedin.com\n",
      "Status code for https://www.github.com is 200\n",
      "Finished getting status of https://www.twitter.com in 0.65 seconds\n",
      "Status code for https://www.twitter.com is 200\n",
      "Finished getting status of https://www.linkedin.com in 0.31 seconds\n",
      "Status code for https://www.linkedin.com is 200\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "def get_status(url):\n",
    "    print(f\"Getting status of {url}\")\n",
    "    start_time = time.monotonic()\n",
    "    response = requests.get(url)\n",
    "    total_time = time.monotonic() - start_time\n",
    "    print(f\"Finished getting status of {url} in {total_time:.2f} seconds\")\n",
    "    return url, response.status_code\n",
    "\n",
    "urls = [\n",
    "    \"https://www.google.com\",\n",
    "    \"https://www.facebook.com\",\n",
    "    \"https://www.twitter.com\",\n",
    "    \"https://www.github.com\",\n",
    "    \"https://www.linkedin.com\"\n",
    "]    \n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    futures = []\n",
    "    for url in urls:\n",
    "        future = executor.submit(get_status, url)\n",
    "        futures.append(future)\n",
    "\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            url, code = future.result()\n",
    "            print(f\"Status code for {url} is {code}\")\n",
    "        except Exception as err:\n",
    "            print(f\"Task failed! {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that:\n",
    "* instead of saving threads in `threads` list, we're going to save these as `futures`. You can think of futures as Python's way representing the result of a threads, kind of like a placeholder for the result.\n",
    "* to create futures we do like we did before: we're going to iterate through our URLs and we're going to create a future by submitting a task with `submit` method to our executor. This is the equivalent of creating threads in the `ThreadPoolExecutor`.\n",
    "* to start the threads, again we're going to iterate over something, in this case `as_completed()` function which we pass in our list of futures and it gives us an iterator that yields the futures as they complete. So, all we have to do is iterate over `concurrent.futures.as_completed(futures)` and then we can get the result as they come in. To get the results our we use a try-except structure.\n",
    "\n",
    "This might be more verbose compared to the previous example, but the biggest advantage now is that we have this \"auto-balance\" for us with `ThreadPoolExecutor` just by specifying `max_workers`, so we can limi the number of active threads that are going on.\n",
    "\n",
    "(*I don't actually understand well the result explanation by the author, until 15.19 min of the video*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a bit of complexity and also explore some more things to consider with threading. Now, we add a functionality where we write the status to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting status of https://www.google.comGetting status of https://www.facebook.com\n",
      "\n",
      "Finished getting status of https://www.facebook.com in 0.20 seconds\n",
      "Status code for https://www.facebook.com is 200\n",
      "Getting status of https://www.twitter.com\n",
      "Finished getting status of https://www.google.com in 0.33 seconds\n",
      "Status code for https://www.google.com is 200\n",
      "Getting status of https://www.github.com\n",
      "Finished getting status of https://www.github.com in 0.30 seconds\n",
      "Status code for https://www.github.com is 200\n",
      "Getting status of https://www.linkedin.com\n",
      "Finished getting status of https://www.twitter.com in 0.66 seconds\n",
      "Status code for https://www.twitter.com is 200\n",
      "Finished getting status of https://www.linkedin.com in 0.38 seconds\n",
      "Status code for https://www.linkedin.com is 200\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import requests\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "STATUS_REPORT = Path(\"status_report.txt\")\n",
    "STATUS_REPORT.write_text(\"\")\n",
    "\n",
    "def get_status(url):\n",
    "    current_text = STATUS_REPORT.read_text()\n",
    "    print(f\"Getting status of {url}\")\n",
    "    start_time = time.monotonic()\n",
    "    response = requests.get(url)\n",
    "    total_time = time.monotonic() - start_time\n",
    "    print(f\"Finished getting status of {url} in {total_time:.2f} seconds\")\n",
    "    STATUS_REPORT.write_text(f\"{current_text}{url}: {response.status_code}\\n\")\n",
    "    return url, response.status_code\n",
    "\n",
    "urls = [\n",
    "    \"https://www.google.com\",\n",
    "    \"https://www.facebook.com\",\n",
    "    \"https://www.twitter.com\",\n",
    "    \"https://www.github.com\",\n",
    "    \"https://www.linkedin.com\"\n",
    "]\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    futures = []\n",
    "    for url in urls:\n",
    "        future = executor.submit(get_status, url)\n",
    "        futures.append(future)\n",
    "\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            url, code = future.result()\n",
    "            print(f\"Status code for {url} is {code}\")\n",
    "        except Exception as err:\n",
    "            print(f\"Task failed! {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the file that was created `status_report.txt`, we can see that is a complete mess: it starts with a blank line, then we have status code about only three services while the remaining ones are missing. What's happening? This is something that we need to keep in mind when working with threads, especially threads that are interacting with the same resources (in this case `STATUS_REPORT`). All these are trying to interact with the same file and this it could get worse if you use `max_workers=5` because again all these threads are fighting of the file overriding what each one is doing. This is not ideal, so we need a way to deal with this.\n",
    "\n",
    "There is a pretty simple mechanism coming from `threading` library. We'll create an instance of `Lock()` class and we can use this to determine when different threads can interact with different things. There are a couple of way yu can deal with this lock:\n",
    "* use `REPORT_LOCK.acquire()` to check if anything else has already acquire the lock and, if not, it will acquire the lock (basically it's saying \"I'm going now, anyone else behind me is gonna have to wait\" then it does all the execution and with `REPORT_LOCK.release()` it's saying \"I'm done, so I'm releasing it for the rest of you\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting status of https://www.google.com\n",
      "Finished getting status of https://www.google.com in 0.35 seconds\n",
      "Status code for https://www.google.com is 200\n",
      "Getting status of https://www.facebook.com\n",
      "Finished getting status of https://www.facebook.com in 0.19 seconds\n",
      "Status code for https://www.facebook.com is 200\n",
      "Getting status of https://www.twitter.com\n",
      "Finished getting status of https://www.twitter.com in 0.70 seconds\n",
      "Getting status of https://www.github.com\n",
      "Status code for https://www.twitter.com is 200\n",
      "Finished getting status of https://www.github.com in 0.30 seconds\n",
      "Getting status of https://www.linkedin.com\n",
      "Status code for https://www.github.com is 200\n",
      "Finished getting status of https://www.linkedin.com in 0.42 seconds\n",
      "Status code for https://www.linkedin.com is 200\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import requests\n",
    "import time\n",
    "from pathlib import Path\n",
    "import threading\n",
    "\n",
    "STATUS_REPORT = Path(\"status_report.txt\")\n",
    "STATUS_REPORT.write_text(\"\")\n",
    "REPORT_LOCK = threading.Lock()\n",
    "\n",
    "def get_status(url):\n",
    "    REPORT_LOCK.acquire()\n",
    "    current_text = STATUS_REPORT.read_text()\n",
    "    print(f\"Getting status of {url}\")\n",
    "    start_time = time.monotonic()\n",
    "    response = requests.get(url)\n",
    "    total_time = time.monotonic() - start_time\n",
    "    print(f\"Finished getting status of {url} in {total_time:.2f} seconds\")\n",
    "    STATUS_REPORT.write_text(f\"{current_text}{url}: {response.status_code}\\n\")\n",
    "    REPORT_LOCK.release()\n",
    "    return url, response.status_code\n",
    "\n",
    "urls = [\n",
    "    \"https://www.google.com\",\n",
    "    \"https://www.facebook.com\",\n",
    "    \"https://www.twitter.com\",\n",
    "    \"https://www.github.com\",\n",
    "    \"https://www.linkedin.com\"\n",
    "]\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    futures = []\n",
    "    for url in urls:\n",
    "        future = executor.submit(get_status, url)\n",
    "        futures.append(future)\n",
    "\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            url, code = future.result()\n",
    "            print(f\"Status code for {url} is {code}\")\n",
    "        except Exception as err:\n",
    "            print(f\"Task failed! {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the output, we can confirm that it works well. Even though this code is valid, it's better to use a context manager:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting status of https://www.google.com\n",
      "Finished getting status of https://www.google.com in 0.34 seconds\n",
      "Status code for https://www.google.com is 200\n",
      "Getting status of https://www.facebook.com\n",
      "Finished getting status of https://www.facebook.com in 0.25 seconds\n",
      "Status code for https://www.facebook.com is 200\n",
      "Getting status of https://www.twitter.com\n",
      "Finished getting status of https://www.twitter.com in 0.71 seconds\n",
      "Status code for https://www.twitter.com is 200\n",
      "Getting status of https://www.github.com\n",
      "Finished getting status of https://www.github.com in 0.27 seconds\n",
      "Getting status of https://www.linkedin.com\n",
      "Status code for https://www.github.com is 200\n",
      "Finished getting status of https://www.linkedin.com in 0.40 seconds\n",
      "Status code for https://www.linkedin.com is 200\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import requests\n",
    "import time\n",
    "from pathlib import Path\n",
    "import threading\n",
    "\n",
    "STATUS_REPORT = Path(\"status_report.txt\")\n",
    "STATUS_REPORT.write_text(\"\")\n",
    "REPORT_LOCK = threading.Lock()\n",
    "\n",
    "def get_status(url):\n",
    "    with REPORT_LOCK:\n",
    "        current_text = STATUS_REPORT.read_text()\n",
    "        print(f\"Getting status of {url}\")\n",
    "        start_time = time.monotonic()\n",
    "        response = requests.get(url)\n",
    "        total_time = time.monotonic() - start_time\n",
    "        print(f\"Finished getting status of {url} in {total_time:.2f} seconds\")\n",
    "        STATUS_REPORT.write_text(f\"{current_text}{url}: {response.status_code}\\n\")\n",
    "    return url, response.status_code\n",
    "\n",
    "urls = [\n",
    "    \"https://www.google.com\",\n",
    "    \"https://www.facebook.com\",\n",
    "    \"https://www.twitter.com\",\n",
    "    \"https://www.github.com\",\n",
    "    \"https://www.linkedin.com\"\n",
    "]\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    futures = []\n",
    "    for url in urls:\n",
    "        future = executor.submit(get_status, url)\n",
    "        futures.append(future)\n",
    "\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            url, code = future.result()\n",
    "            print(f\"Status code for {url} is {code}\")\n",
    "        except Exception as err:\n",
    "            print(f\"Task failed! {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the output of the print statements, we can see that this has effectively made it sequential (first Google, second Facebook, and so on.). That's because we are blocking thing for other threads at the very top our function. If you wanted to design this better, we could collapse the lock to only the context that it needs to be in. So, everything else that doens't need that lock context can happen outside of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting status of https://www.google.com\n",
      "Getting status of https://www.facebook.com\n",
      "Getting status of https://www.twitter.com\n",
      "Getting status of https://www.github.com\n",
      "Getting status of https://www.linkedin.com\n",
      "Finished getting status of https://www.facebook.com in 0.23 seconds\n",
      "Status code for https://www.facebook.com is 200\n",
      "Finished getting status of https://www.github.com in 0.37 seconds\n",
      "Status code for https://www.github.com is 200\n",
      "Finished getting status of https://www.google.com in 0.41 seconds\n",
      "Status code for https://www.google.com is 200\n",
      "Finished getting status of https://www.linkedin.com in 0.45 seconds\n",
      "Status code for https://www.linkedin.com is 200\n",
      "Finished getting status of https://www.twitter.com in 0.72 seconds\n",
      "Status code for https://www.twitter.com is 200\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import requests\n",
    "import time\n",
    "from pathlib import Path\n",
    "import threading\n",
    "\n",
    "STATUS_REPORT = Path(\"status_report.txt\")\n",
    "STATUS_REPORT.write_text(\"\")\n",
    "REPORT_LOCK = threading.Lock()\n",
    "\n",
    "def get_status(url):\n",
    "    print(f\"Getting status of {url}\")\n",
    "    start_time = time.monotonic()\n",
    "    response = requests.get(url)\n",
    "    total_time = time.monotonic() - start_time\n",
    "    print(f\"Finished getting status of {url} in {total_time:.2f} seconds\")\n",
    "    with REPORT_LOCK:\n",
    "        current_text = STATUS_REPORT.read_text()\n",
    "        STATUS_REPORT.write_text(f\"{current_text}{url}: {response.status_code}\\n\")\n",
    "    return url, response.status_code\n",
    "\n",
    "urls = [\n",
    "    \"https://www.google.com\",\n",
    "    \"https://www.facebook.com\",\n",
    "    \"https://www.twitter.com\",\n",
    "    \"https://www.github.com\",\n",
    "    \"https://www.linkedin.com\"\n",
    "]\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    futures = []\n",
    "    for url in urls:\n",
    "        future = executor.submit(get_status, url)\n",
    "        futures.append(future)\n",
    "\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            url, code = future.result()\n",
    "            print(f\"Status code for {url} is {code}\")\n",
    "        except Exception as err:\n",
    "            print(f\"Task failed! {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TO FINISH*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
