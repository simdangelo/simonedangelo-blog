{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "import pyspark\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0. Set-Ups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/12 16:31:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark: SparkSession = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Partitioning 1\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdf_generator1(num_iter: int = 1) -> DataFrame:\n",
    "    d = [\n",
    "        {\"a\":\"a\", \"b\": 1},\n",
    "        {\"a\":\"b\", \"b\": 2},\n",
    "        {\"a\":\"c\", \"b\": 3},\n",
    "        {\"a\":\"d\", \"b\": 4},\n",
    "        {\"a\":\"e\", \"b\": 5},\n",
    "        {\"a\":\"e\", \"b\": 6},\n",
    "        {\"a\":\"f\", \"b\": 7},\n",
    "        {\"a\":\"g\", \"b\": 8},\n",
    "        {\"a\":\"h\", \"b\": 9},\n",
    "        {\"a\":\"i\", \"b\": 10},\n",
    "    ]\n",
    "\n",
    "    data = []\n",
    "    for _ in range(0, num_iter):\n",
    "        data.extend(d)\n",
    "    ddl_schema = \"a string, b int\"\n",
    "    df = spark.createDataFrame(data, schema=ddl_schema)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdf_generator2(num_rows: int, num_partitions: int = None) -> DataFrame:\n",
    "    return (\n",
    "        spark.range(num_rows, numPartitions=num_partitions)\n",
    "        .withColumn(\"date\", f.current_date())\n",
    "        .withColumn(\"timestamp\",f.current_timestamp())\n",
    "        .withColumn(\"idstring\", f.col(\"id\").cast(\"string\"))\n",
    "        .withColumn(\"idfirst\", f.col(\"idstring\").substr(0,1))\n",
    "        .withColumn(\"idlast\", f.col(\"idstring\").substr(-1,1))\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Partition Size based on Cores and Data Amount with spark.CreateDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_gen1 = sdf_generator1(2)\n",
    "sdf_gen1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------------+\n",
      "|  a|  b|partition_id|\n",
      "+---+---+------------+\n",
      "|  a|  1|           0|\n",
      "|  b|  2|           0|\n",
      "|  c|  3|           0|\n",
      "|  d|  4|           0|\n",
      "|  e|  5|           0|\n",
      "|  e|  6|           1|\n",
      "|  f|  7|           1|\n",
      "|  g|  8|           1|\n",
      "|  h|  9|           1|\n",
      "|  i| 10|           1|\n",
      "|  a|  1|           2|\n",
      "|  b|  2|           2|\n",
      "|  c|  3|           2|\n",
      "|  d|  4|           2|\n",
      "|  e|  5|           2|\n",
      "|  e|  6|           3|\n",
      "|  f|  7|           3|\n",
      "|  g|  8|           3|\n",
      "|  h|  9|           3|\n",
      "|  i| 10|           3|\n",
      "+---+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_part1 = sdf_gen1.withColumn(\"partition_id\", f.spark_partition_id())\n",
    "sdf_part1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+----------+\n",
      "|partition_id|count|count_perc|\n",
      "+------------+-----+----------+\n",
      "|           0|    5|      25.0|\n",
      "|           1|    5|      25.0|\n",
      "|           2|    5|      25.0|\n",
      "|           3|    5|      25.0|\n",
      "+------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row_count = sdf_gen1.count()\n",
    "sdf_part_count1 = sdf_part1.groupBy(\"partition_id\").count()\n",
    "sdf_part_count1 = sdf_part_count1.withColumn(\"count_perc\", 100*f.col(\"count\")/row_count)\n",
    "sdf_part_count1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Gen1_Exp1\")\n",
    "sdf_gen1.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_gen1_2 = sdf_generator1(2000)\n",
    "sdf_gen1_2.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+----------+\n",
      "|partition_id|count|count_perc|\n",
      "+------------+-----+----------+\n",
      "|           0| 5120|      25.6|\n",
      "|           1| 5120|      25.6|\n",
      "|           2| 5120|      25.6|\n",
      "|           3| 4640|      23.2|\n",
      "+------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_part1_2 = sdf_gen1_2.withColumn(\"partition_id\", f.spark_partition_id())\n",
    "row_count = sdf_gen1_2.count()\n",
    "sdf_part_count1_2 = sdf_part1_2.groupBy(\"partition_id\").count()\n",
    "sdf_part_count1_2 = sdf_part_count1_2.withColumn(\"count_perc\", 100*f.col(\"count\")/row_count)\n",
    "sdf_part_count1_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Gen1_Exp2\")\n",
    "sdf_gen1_2.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Partition Size based on Cores and Data Amount with spark.range\n",
    "The same results as for spark.createDataFrame count also here even though it's a spark function returning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_gen2 = sdf_generator2(2000000)\n",
    "sdf_gen2.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+-------+------+------------+\n",
      "| id|      date|           timestamp|idstring|idfirst|idlast|partition_id|\n",
      "+---+----------+--------------------+--------+-------+------+------------+\n",
      "|  0|2024-08-12|2024-08-12 16:31:...|       0|      0|     0|           0|\n",
      "|  1|2024-08-12|2024-08-12 16:31:...|       1|      1|     1|           0|\n",
      "|  2|2024-08-12|2024-08-12 16:31:...|       2|      2|     2|           0|\n",
      "|  3|2024-08-12|2024-08-12 16:31:...|       3|      3|     3|           0|\n",
      "|  4|2024-08-12|2024-08-12 16:31:...|       4|      4|     4|           0|\n",
      "|  5|2024-08-12|2024-08-12 16:31:...|       5|      5|     5|           0|\n",
      "|  6|2024-08-12|2024-08-12 16:31:...|       6|      6|     6|           0|\n",
      "|  7|2024-08-12|2024-08-12 16:31:...|       7|      7|     7|           0|\n",
      "|  8|2024-08-12|2024-08-12 16:31:...|       8|      8|     8|           0|\n",
      "|  9|2024-08-12|2024-08-12 16:31:...|       9|      9|     9|           0|\n",
      "| 10|2024-08-12|2024-08-12 16:31:...|      10|      1|     0|           0|\n",
      "| 11|2024-08-12|2024-08-12 16:31:...|      11|      1|     1|           0|\n",
      "| 12|2024-08-12|2024-08-12 16:31:...|      12|      1|     2|           0|\n",
      "| 13|2024-08-12|2024-08-12 16:31:...|      13|      1|     3|           0|\n",
      "| 14|2024-08-12|2024-08-12 16:31:...|      14|      1|     4|           0|\n",
      "| 15|2024-08-12|2024-08-12 16:31:...|      15|      1|     5|           0|\n",
      "| 16|2024-08-12|2024-08-12 16:31:...|      16|      1|     6|           0|\n",
      "| 17|2024-08-12|2024-08-12 16:31:...|      17|      1|     7|           0|\n",
      "| 18|2024-08-12|2024-08-12 16:31:...|      18|      1|     8|           0|\n",
      "| 19|2024-08-12|2024-08-12 16:31:...|      19|      1|     9|           0|\n",
      "+---+----------+--------------------+--------+-------+------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_part2 = sdf_gen2.withColumn(\"partition_id\", f.spark_partition_id())\n",
    "sdf_part2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000\n"
     ]
    }
   ],
   "source": [
    "row_count = sdf_gen2.count()\n",
    "print(row_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+----------+\n",
      "|partition_id| count|count_perc|\n",
      "+------------+------+----------+\n",
      "|           0|500000|      25.0|\n",
      "|           1|500000|      25.0|\n",
      "|           2|500000|      25.0|\n",
      "|           3|500000|      25.0|\n",
      "+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_part_count2 = sdf_part2.groupBy(\"partition_id\").count()\n",
    "sdf_part_count2 = sdf_part_count2.withColumn(\"count_perc\", 100*f.col(\"count\")/row_count)\n",
    "sdf_part_count2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Gen2_Exp1\")\n",
    "sdf_gen2.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_gen2_2 = sdf_generator2(2000000000000000000)\n",
    "sdf_gen2_2.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Influence on Spark partitions to the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sdf1 = sdf_generator2(20000000, 4)\n",
    "print(sdf1.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Part Exp1\")\n",
    "sdf1.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sdf4 = sdf_generator2(20000000, 4)\n",
    "print(sdf4.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Part Exp4\")\n",
    "sdf4.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")\n",
    "\n",
    "sdf8 = sdf_generator2(20000000, 8)\n",
    "print(sdf8.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Part Exp8\")\n",
    "sdf8.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")\n",
    "\n",
    "sdf3 = sdf_generator2(20000000, 3)\n",
    "print(sdf3.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Part Exp3\")\n",
    "sdf3.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")\n",
    "\n",
    "sdf6 = sdf_generator2(20000000, 6)\n",
    "print(sdf6.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Part Exp6\")\n",
    "sdf6.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")\n",
    "\n",
    "sdf200 = sdf_generator2(20000000, 200)\n",
    "print(sdf200.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Part Exp200\")\n",
    "sdf200.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")\n",
    "\n",
    "sdf20000 = sdf_generator2(20000000, 20000)\n",
    "print(sdf20000.rdd.getNumPartitions())\n",
    "sc.setJobDescription(\"Part Exp20000\")\n",
    "sdf20000.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
