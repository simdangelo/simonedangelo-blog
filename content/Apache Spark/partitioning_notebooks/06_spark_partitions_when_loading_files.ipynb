{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Today's topic: How Spark partitions are influenced when loading the data with parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/simonedangelo/Documents/simonedangelo-blog/myenv/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/simonedangelo/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/simonedangelo/.ivy2/jars\n",
      "uk.co.gresearch.spark#spark-extension_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ccf0cad9-0e17-4d84-bbbe-4fadc91b418b;1.0\n",
      "\tconfs: [default]\n",
      "\tfound uk.co.gresearch.spark#spark-extension_2.12;2.11.0-3.5 in central\n",
      "\tfound com.github.scopt#scopt_2.12;4.1.0 in central\n",
      ":: resolution report :: resolve 118ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.scopt#scopt_2.12;4.1.0 from central in [default]\n",
      "\tuk.co.gresearch.spark#spark-extension_2.12;2.11.0-3.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ccf0cad9-0e17-4d84-bbbe-4fadc91b418b\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/4ms)\n",
      "24/12/28 12:46:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Data with Nikk the Greek Spark Session\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .config(\"spark.jars.packages\", \"uk.co.gresearch.spark:spark-extension_2.12:2.11.0-3.5\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\"\"\"\n",
    "Reference gresearch:\n",
    "- Parquet files analysis: https://www.gresearch.com/blog/article/parquet-files-know-your-scaling-limits/\n",
    "- GitHub Spark extension: https://github.com/G-Research/spark-extension\n",
    "- Parquet methods: https://github.com/G-Research/spark-extension/tree/master/python/gresearch/spark/parquet\n",
    "\"\"\"\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gresearch.spark.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdf_generator(num_rows: int, num_partitions: int = None) -> \"DataFrame\":\n",
    "    return (\n",
    "        spark.range(num_rows, numPartitions=num_partitions)\n",
    "        .withColumn(\"date\", f.current_date())\n",
    "        .withColumn(\"timestamp\",f.current_timestamp())\n",
    "        .withColumn(\"idstring\", f.col(\"id\").cast(\"string\"))\n",
    "        .withColumn(\"idfirst\", f.col(\"idstring\").substr(0,1))\n",
    "        .withColumn(\"idlast\", f.col(\"idstring\").substr(-1,1))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"base_dir_loading_lesson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "results_list = []\n",
    "def write_generator(num_rows, num_files):\n",
    "    sdf = sdf_generator(num_rows, num_files)\n",
    "    path = f\"{BASE_DIR}/{num_files}_files_{num_rows}_rows.parquet\"\n",
    "    sc.setJobDescription(f\"Write {num_files} files, {num_rows} rows\")\n",
    "    sdf.write.format(\"parquet\").mode(\"overwrite\").save(path)\n",
    "    sc.setJobDescription(\"None\")\n",
    "    print(f\"Num partitions written: {sdf.rdd.getNumPartitions()}\")\n",
    "    print(f\"Saved Path: {path}\")\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_configs(maxPartitionsMB = 128, openCostInMB = 4, minPartitions = 4):\n",
    "    maxPartitionsBytes = math.ceil(maxPartitionsMB*1024*1024)\n",
    "    openCostInBytes = math.ceil(openCostInMB*1024*1024)\n",
    "    spark.conf.set(\"spark.sql.files.maxPartitionBytes\", str(maxPartitionsBytes)+\"b\")\n",
    "    spark.conf.set(\"spark.sql.files.openCostInBytes\", str(openCostInBytes)+\"b\")\n",
    "    spark.conf.set(\"spark.sql.files.minPartitionNum\", str(minPartitions))\n",
    "    print(\" \")\n",
    "    print(\"******** SPARK CONFIGURATIONS ********\")\n",
    "    print(f\"MaxPartitionSize {maxPartitionsMB} MB or {maxPartitionsBytes} bytes\")\n",
    "    print(f\"OpenCostInBytes {openCostInMB} MB or {openCostInBytes} bytes\")\n",
    "    print(f\"Min Partitions: {minPartitions}\")\n",
    "\n",
    "    results_dict[\"maxPartitionsBytes\"] = maxPartitionsMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bytes_rows_per_partition(path):\n",
    "    sdf = (\n",
    "        spark.read.parquet_partitions(path)\n",
    "        .groupBy(\"partition\").agg(f.sum(\"compressedBytes\"), f.sum(\"rows\"), f.count(\"partition\"))\n",
    "        .withColumnRenamed(\"sum(compressedBytes)\", \"compressedBytes\")\n",
    "        .withColumnRenamed(\"sum(rows)\", \"rows\")\n",
    "        .withColumnRenamed(\"count(partition)\", \"numFiles\")\n",
    "        .withColumn(\"compressedMB\", f.round(f.col(\"compressedBytes\")/1024/1024, 1))\n",
    "        .select(\"partition\", \"numFiles\", \"compressedBytes\",\"compressedMB\",\"rows\")\n",
    "        .orderBy(\"partition\")\n",
    "    )\n",
    "    sdf.show(20)\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Basic algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic algorithm\n",
    "def basic_algorithm(file_size):\n",
    "    maxPartitionBytes = int(spark.conf.get(\"spark.sql.files.maxPartitionBytes\")[:-1])    \n",
    "    minPartitionNum = int(spark.conf.get(\"spark.sql.files.minPartitionNum\"))\n",
    "    size_per_core = file_size/minPartitionNum\n",
    "    partition_size = min(maxPartitionBytes, size_per_core)\n",
    "    no_partitions = file_size/partition_size #round up for no_partitions\n",
    "    \n",
    "    print(\" \")\n",
    "    print(\"******** BASIC ALGORITHM TO ESTIMATE NO PARTITIONS ********\")\n",
    "    print(f\"File Size: {round(file_size/1024/1024, 1)} MB or {file_size} bytes\")\n",
    "    print(f\"Size Per Core: {round(size_per_core/1024/1024, 1)} MB or {size_per_core} bytes\")\n",
    "    print(f\"Partion size: {round(partition_size/1024/1024, 1)} MB or {partition_size} bytes\")\n",
    "    print(f\"EstimatedPartitions: {math.ceil(no_partitions)}, unrounded: {no_partitions}\")\n",
    "\n",
    "#Reference: https://www.linkedin.com/pulse/how-initial-number-partitions-determined-pyspark-sugumar-srinivasan#:~:text=Ideally%20partitions%20will%20be%20created,resource%20will%20get%20utilised%20properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** BASIC ALGORITHM TO ESTIMATE NO PARTITIONS ********\n",
      "File Size: 64.0 MB or 67108864 bytes\n",
      "Size Per Core: 16.0 MB or 16777216.0 bytes\n",
      "Partion size: 16.0 MB or 16777216.0 bytes\n",
      "EstimatedPartitions: 4, unrounded: 4.0\n"
     ]
    }
   ],
   "source": [
    "file_size = 64\n",
    "set_configs(maxPartitionsMB=128, openCostInMB=4, minPartitions=4)\n",
    "basic_algorithm(file_size*1024*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** BASIC ALGORITHM TO ESTIMATE NO PARTITIONS ********\n",
      "File Size: 100.0 MB or 104857600 bytes\n",
      "Size Per Core: 25.0 MB or 26214400.0 bytes\n",
      "Partion size: 25.0 MB or 26214400.0 bytes\n",
      "EstimatedPartitions: 4, unrounded: 4.0\n"
     ]
    }
   ],
   "source": [
    "file_size = 100\n",
    "set_configs(maxPartitionsMB=128, openCostInMB=4, minPartitions=4)\n",
    "basic_algorithm(file_size*1024*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 128 MB or 134217728 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** BASIC ALGORITHM TO ESTIMATE NO PARTITIONS ********\n",
      "File Size: 200.0 MB or 209715200 bytes\n",
      "Size Per Core: 50.0 MB or 52428800.0 bytes\n",
      "Partion size: 50.0 MB or 52428800.0 bytes\n",
      "EstimatedPartitions: 4, unrounded: 4.0\n"
     ]
    }
   ],
   "source": [
    "file_size = 200\n",
    "set_configs(maxPartitionsMB=128, openCostInMB=4, minPartitions=4)\n",
    "basic_algorithm(file_size*1024*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 45 MB or 47185920 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** BASIC ALGORITHM TO ESTIMATE NO PARTITIONS ********\n",
      "File Size: 200.0 MB or 209715200 bytes\n",
      "Size Per Core: 50.0 MB or 52428800.0 bytes\n",
      "Partion size: 45.0 MB or 47185920 bytes\n",
      "EstimatedPartitions: 5, unrounded: 4.444444444444445\n"
     ]
    }
   ],
   "source": [
    "file_size = 200\n",
    "set_configs(maxPartitionsMB=45, openCostInMB=4, minPartitions=4)\n",
    "basic_algorithm(file_size*1024*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 1 MB or 1048576 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** BASIC ALGORITHM TO ESTIMATE NO PARTITIONS ********\n",
      "File Size: 200.0 MB or 209715200 bytes\n",
      "Size Per Core: 50.0 MB or 52428800.0 bytes\n",
      "Partion size: 1.0 MB or 1048576 bytes\n",
      "EstimatedPartitions: 200, unrounded: 200.0\n"
     ]
    }
   ],
   "source": [
    "file_size = 200\n",
    "set_configs(maxPartitionsMB=1, openCostInMB=4, minPartitions=4)\n",
    "basic_algorithm(file_size*1024*1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Simple experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Experiment 1: 4 files, a 64.8 MB, sum 259.3 MB\n",
    "* Experiment 2: 8 files, a 64.9 MB MB, sum 518.9 MB\n",
    "* Experiment 3: 8 files, a 47,5 MB, sum 380 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parquet_file_size(path):\n",
    "    sdf = (\n",
    "        spark.read.parquet_metadata(path)\n",
    "        .select(\"filename\", \"blocks\", \"compressedBytes\", \"rows\")\n",
    "        .dropDuplicates([\"filename\"])\n",
    "    )\n",
    "    sum = sdf.select(f.sum(sdf[\"compressedBytes\"]))\n",
    "    size = sum.collect()[0][0]\n",
    "    return size\n",
    "\n",
    "def file_analysis(path, num_files):\n",
    "    file_size = get_parquet_file_size(path)\n",
    "    avg_file_size = file_size/num_files\n",
    "    print(\" \")\n",
    "    print(\"******** FILE SIZE ANALYSIS ********\")\n",
    "    print(f\"File Size: {round(file_size/1024/1024, 1)} MB or {file_size} bytes\")\n",
    "    print(f\"Num files: {num_files}\")\n",
    "    print(f\"Avg file Size: {round(avg_file_size/1024/1024, 1)} MB or {avg_file_size} bytes\")\n",
    "\n",
    "def row_count_analysis(num_files, num_rows):\n",
    "    print(\" \")\n",
    "    print(\"******** ROW COUNT ANALYSIS ********\")    \n",
    "    print(f\"Num files written: {num_files}\")\n",
    "    print(f\"Num rows written: {num_rows}\")\n",
    "    print(f\"Num rows per file: {int(num_rows/num_files)}\")\n",
    "\n",
    "def estimate_num_partitions(file_size, num_files):\n",
    "    \"\"\"\n",
    "    Reference to code: \n",
    "    - Stackoverflow: https://stackoverflow.com/questions/70985235/what-is-opencostinbytes\n",
    "    - GitHub: https://github.com/apache/spark/blob/v3.3.1/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FilePartition.scala#L86-L97\n",
    "    \"\"\"\n",
    "    #get spark values\n",
    "    maxPartitionBytes = int(spark.conf.get(\"spark.sql.files.maxPartitionBytes\")[:-1])\n",
    "    openCostInBytes = int(spark.conf.get(\"spark.sql.files.openCostInBytes\")[:-1])\n",
    "    minPartitionNum = int(spark.conf.get(\"spark.sql.files.minPartitionNum\"))\n",
    "    #Calculate maxSpliPartitionBytes\n",
    "    # a) If we have bigger files bytesPerCorePadded will be bigger then openCostInBytes but also maxPartitionBytes. \n",
    "    # In this case we would limit the size as mayPartitionBytes will be the result of maxSplitPartitionBytes. e.g. 1 GB dataset, 4 cores, maxPartitions 128 MB\n",
    "    # b) If bytesPerCorePadded is the result of maxSplitPartitionBytes we have a fair split of the data over all cores, e.g. 1 GB dataset, 4 cores, maxPartitions 300 MB\n",
    "    # c) If bytesPerCore is to small we want to limit amount of Partitions to be opened. This is the cost here.\n",
    "    paddedFileSize = file_size + num_files * openCostInBytes\n",
    "    bytesPerCorePadded = paddedFileSize / minPartitionNum\n",
    "    maxSplitPartitionBytes = min(maxPartitionBytes, max(openCostInBytes, bytesPerCorePadded))\n",
    "    #Estimation of partitions from Internet\n",
    "    estimated_num_partitions_int = paddedFileSize/maxSplitPartitionBytes\n",
    "    #Own Estimator\n",
    "    avg_file_size_padded = paddedFileSize/num_files\n",
    "    bytesPerCore = file_size / minPartitionNum\n",
    "    #Calculate number of files fitting into one partitions. Then calculate the number of partitions\n",
    "    files_per_partition = max(1, math.floor(maxSplitPartitionBytes/avg_file_size_padded))\n",
    "    estimated_num_partitions = num_files/files_per_partition\n",
    "    print(\" \")\n",
    "    print(\"******** ESTIMATION OF MAX SPLIT PARTITION BYTES AND NO PARTITIONS ********\")\n",
    "    print(f\"Avg file Size Padded: {round(avg_file_size_padded/1024/1024, 1)} MB or {avg_file_size_padded} bytes\")\n",
    "    print(f\"Padded File Size: {round(paddedFileSize/1024/1024, 1)} MB or {paddedFileSize} bytes\")\n",
    "    print(f\"SizePerCore: {round(bytesPerCore/1024/1024, 1)} MB or {bytesPerCore} bytes\")\n",
    "    print(f\"SizePerCorePadded: {round(bytesPerCorePadded/1024/1024, 1)} MB or {bytesPerCorePadded} bytes\")\n",
    "    print(f\"MaxSplitPartitionBytes: {round(maxSplitPartitionBytes/1024/1024, 1)} MB or {maxSplitPartitionBytes} bytes\")\n",
    "    print(f\"MaxFilesPerPartition {files_per_partition}\")\n",
    "    print(f\"EstimatedPartitions: {math.ceil(estimated_num_partitions)}, unrounded: {estimated_num_partitions}\")\n",
    "    print(f\"EstimatedPartitionsInternet: {math.ceil(estimated_num_partitions_int)}, unrounded: {estimated_num_partitions_int}\")\n",
    "\n",
    "    results_dict[\"paddedFileSize\"] = round(paddedFileSize/1024/1024, 1)\n",
    "    results_dict[\"MBPerCore\"] = round(bytesPerCore/1024/1024, 1)\n",
    "    results_dict[\"MBPerCorePadded\"] = round(bytesPerCorePadded/1024/1024, 1)\n",
    "    results_dict[\"maxSplitPartitionBytes\"] = round(maxSplitPartitionBytes/1024/1024, 1)\n",
    "    results_dict[\"avg_file_size_padded\"] = round(avg_file_size_padded/1024/1024, 1)\n",
    "    results_dict[\"Maxfiles_per_partition\"] = files_per_partition\n",
    "    results_dict[\"MyEstimationPartitions\"] = math.ceil(estimated_num_partitions)\n",
    "    results_dict[\"InternetEstimationPartitions\"] = math.ceil(estimated_num_partitions_int)\n",
    "\n",
    "def get_actual_num_partitions(path):\n",
    "    sdf = spark.read.parquet(path)\n",
    "    print(\" \")\n",
    "    print(\"******** ACTUAL RESULTS ********\")   \n",
    "    print(f\"ActualNumPartitions: {sdf.rdd.getNumPartitions()}\")\n",
    "    results_dict[\"ActualNumPartitions\"] = sdf.rdd.getNumPartitions()\n",
    "\n",
    "def noop_write(path):\n",
    "    sdf = spark.read.parquet(path)\n",
    "    sc.setJobDescription(\"WRITE\")\n",
    "    start_time = time.time()\n",
    "    sdf.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "    end_time = time.time()\n",
    "    sc.setJobDescription(\"None\")\n",
    "    duration = round(end_time - start_time, 2)\n",
    "    results_dict[\"ExecutionTime\"] = duration\n",
    "    print(f\"Duration: {duration} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"../../base_dir_loading_lesson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:==============>                                            (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num partitions written: 4\n",
      "Saved Path: ../../base_dir_loading_lesson/4_files_32000000_rows.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "num_files = 4\n",
    "num_rows = 32000000\n",
    "path = write_generator(num_rows, num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "******** FILE SIZE ANALYSIS ********\n",
      "File Size: 267.6 MB or 280634098 bytes\n",
      "Num files: 4\n",
      "Avg file Size: 66.9 MB or 70158524.5 bytes\n",
      " \n",
      "******** ROW COUNT ANALYSIS ********\n",
      "Num files written: 4\n",
      "Num rows written: 32000000\n",
      "Num rows per file: 8000000\n",
      " \n",
      "******** SPARK CONFIGURATIONS ********\n",
      "MaxPartitionSize 50 MB or 52428800 bytes\n",
      "OpenCostInBytes 4 MB or 4194304 bytes\n",
      "Min Partitions: 4\n",
      " \n",
      "******** BASIC ALGORITHM TO ESTIMATE NO PARTITIONS ********\n",
      "File Size: 267.6 MB or 280634098 bytes\n",
      "Size Per Core: 66.9 MB or 70158524.5 bytes\n",
      "Partion size: 50.0 MB or 52428800 bytes\n",
      "EstimatedPartitions: 6, unrounded: 5.352670631408691\n",
      " \n",
      "******** ESTIMATION OF MAX SPLIT PARTITION BYTES AND NO PARTITIONS ********\n",
      "Avg file Size Padded: 70.9 MB or 74352828.5 bytes\n",
      "Padded File Size: 283.6 MB or 297411314 bytes\n",
      "SizePerCore: 66.9 MB or 70158524.5 bytes\n",
      "SizePerCorePadded: 70.9 MB or 74352828.5 bytes\n",
      "MaxSplitPartitionBytes: 50.0 MB or 52428800 bytes\n",
      "MaxFilesPerPartition 1\n",
      "EstimatedPartitions: 4, unrounded: 4.0\n",
      "EstimatedPartitionsInternet: 6, unrounded: 5.672670631408692\n",
      " \n",
      "******** ACTUAL RESULTS ********\n",
      "ActualNumPartitions: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 1.98 sec\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|partition|numFiles|compressedBytes|compressedMB|   rows|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "|        0|       1|       70205760|        67.0|8000000|\n",
      "|        1|       1|       69631609|        66.4|8000000|\n",
      "|        2|       1|       70398611|        67.1|8000000|\n",
      "|        3|       1|       70398118|        67.1|8000000|\n",
      "|        4|       2|              0|         0.0|      0|\n",
      "|        5|       2|              0|         0.0|      0|\n",
      "+---------+--------+---------------+------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[partition: int, numFiles: bigint, compressedBytes: bigint, compressedMB: double, rows: bigint]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../../base_dir_loading_lesson/4_files_32000000_rows.parquet\"\n",
    "num_files = 4\n",
    "num_rows = 32000000\n",
    "file_analysis(path, num_files)\n",
    "row_count_analysis(num_files, num_rows)\n",
    "set_configs(maxPartitionsMB=50, openCostInMB=4, minPartitions=4)\n",
    "size = get_parquet_file_size(path)\n",
    "basic_algorithm(size)\n",
    "estimate_num_partitions(size, num_files)\n",
    "get_actual_num_partitions(path)\n",
    "noop_write(path)\n",
    "bytes_rows_per_partition(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
