{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/11 20:34:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Repartition\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdf_generator(num_rows: int, num_partitions: int = None) -> DataFrame:\n",
    "    return (\n",
    "        spark.range(num_rows, numPartitions=num_partitions)\n",
    "        .withColumn(\"date\", f.current_date())\n",
    "        .withColumn(\"timestamp\",f.current_timestamp())\n",
    "        .withColumn(\"idstring\", f.col(\"id\").cast(\"string\"))\n",
    "        .withColumn(\"idfirst\", f.col(\"idstring\").substr(0,1))\n",
    "        .withColumn(\"idlast\", f.col(\"idstring\").substr(-1,1))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_gen = sdf_generator(20)\n",
    "sdf_gen.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+-------+------+\n",
      "| id|      date|           timestamp|idstring|idfirst|idlast|\n",
      "+---+----------+--------------------+--------+-------+------+\n",
      "|  0|2024-12-11|2024-12-11 20:34:...|       0|      0|     0|\n",
      "|  1|2024-12-11|2024-12-11 20:34:...|       1|      1|     1|\n",
      "|  2|2024-12-11|2024-12-11 20:34:...|       2|      2|     2|\n",
      "|  3|2024-12-11|2024-12-11 20:34:...|       3|      3|     3|\n",
      "|  4|2024-12-11|2024-12-11 20:34:...|       4|      4|     4|\n",
      "|  5|2024-12-11|2024-12-11 20:34:...|       5|      5|     5|\n",
      "|  6|2024-12-11|2024-12-11 20:34:...|       6|      6|     6|\n",
      "|  7|2024-12-11|2024-12-11 20:34:...|       7|      7|     7|\n",
      "|  8|2024-12-11|2024-12-11 20:34:...|       8|      8|     8|\n",
      "|  9|2024-12-11|2024-12-11 20:34:...|       9|      9|     9|\n",
      "| 10|2024-12-11|2024-12-11 20:34:...|      10|      1|     0|\n",
      "| 11|2024-12-11|2024-12-11 20:34:...|      11|      1|     1|\n",
      "| 12|2024-12-11|2024-12-11 20:34:...|      12|      1|     2|\n",
      "| 13|2024-12-11|2024-12-11 20:34:...|      13|      1|     3|\n",
      "| 14|2024-12-11|2024-12-11 20:34:...|      14|      1|     4|\n",
      "| 15|2024-12-11|2024-12-11 20:34:...|      15|      1|     5|\n",
      "| 16|2024-12-11|2024-12-11 20:34:...|      16|      1|     6|\n",
      "| 17|2024-12-11|2024-12-11 20:34:...|      17|      1|     7|\n",
      "| 18|2024-12-11|2024-12-11 20:34:...|      18|      1|     8|\n",
      "| 19|2024-12-11|2024-12-11 20:34:...|      19|      1|     9|\n",
      "+---+----------+--------------------+--------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_gen.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+----------+\n",
      "|partition_id|count|count_perc|\n",
      "+------------+-----+----------+\n",
      "|           0|    5|      25.0|\n",
      "|           1|    5|      25.0|\n",
      "|           2|    5|      25.0|\n",
      "|           3|    5|      25.0|\n",
      "+------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def rows_per_partition(sdf: \"DataFrame\", num_rows: int) -> None:\n",
    "    sdf_part = sdf.withColumn(\"partition_id\", f.spark_partition_id())\n",
    "    sdf_part_count = sdf_part.groupBy(\"partition_id\").count()\n",
    "    sdf_part_count = sdf_part_count.withColumn(\"count_perc\", 100*f.col(\"count\")/num_rows)\n",
    "    sdf_part_count.orderBy(\"partition_id\").show()\n",
    "\n",
    "rows_per_partition(sdf_gen, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-----+----------+\n",
      "|partition_id|idfirst|count|count_perc|\n",
      "+------------+-------+-----+----------+\n",
      "|           0|      0|    1|       5.0|\n",
      "|           0|      1|    1|       5.0|\n",
      "|           0|      2|    1|       5.0|\n",
      "|           0|      3|    1|       5.0|\n",
      "|           0|      4|    1|       5.0|\n",
      "|           1|      5|    1|       5.0|\n",
      "|           1|      6|    1|       5.0|\n",
      "|           1|      7|    1|       5.0|\n",
      "|           1|      8|    1|       5.0|\n",
      "|           1|      9|    1|       5.0|\n",
      "|           2|      1|    5|      25.0|\n",
      "|           3|      1|    5|      25.0|\n",
      "+------------+-------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def rows_per_partition_col(sdf: \"DataFrame\", num_rows: int, col: str) -> None:\n",
    "    sdf_part = sdf.withColumn(\"partition_id\", f.spark_partition_id())\n",
    "    sdf_part_count = sdf_part.groupBy(\"partition_id\", col).count()\n",
    "    sdf_part_count = sdf_part_count.withColumn(\"count_perc\", 100*f.col(\"count\")/num_rows)\n",
    "    sdf_part_count.orderBy(\"partition_id\", col).show()\n",
    "\n",
    "rows_per_partition_col(sdf_gen, 20, \"idfirst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf1 = sdf_generator(num_rows=num_rows, num_partitions=4)\n",
    "sdf1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "row_count = sdf1.count()\n",
    "print(row_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+----------+\n",
      "|partition_id|count|count_perc|\n",
      "+------------+-----+----------+\n",
      "|           0| 5000|      25.0|\n",
      "|           1| 5000|      25.0|\n",
      "|           2| 5000|      25.0|\n",
      "|           3| 5000|      25.0|\n",
      "+------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows_per_partition(sdf1, num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-----+----------+\n",
      "|partition_id|idfirst|count|count_perc|\n",
      "+------------+-------+-----+----------+\n",
      "|           0|      0|    1|     0.005|\n",
      "|           0|      1| 1111|     5.555|\n",
      "|           0|      2| 1111|     5.555|\n",
      "|           0|      3| 1111|     5.555|\n",
      "|           0|      4| 1111|     5.555|\n",
      "|           0|      5|  111|     0.555|\n",
      "|           0|      6|  111|     0.555|\n",
      "|           0|      7|  111|     0.555|\n",
      "|           0|      8|  111|     0.555|\n",
      "|           0|      9|  111|     0.555|\n",
      "|           1|      5| 1000|       5.0|\n",
      "|           1|      6| 1000|       5.0|\n",
      "|           1|      7| 1000|       5.0|\n",
      "|           1|      8| 1000|       5.0|\n",
      "|           1|      9| 1000|       5.0|\n",
      "|           2|      1| 5000|      25.0|\n",
      "|           3|      1| 5000|      25.0|\n",
      "+------------+-------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows_per_partition_col(sdf1, num_rows, \"idfirst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Baseline 4 partitions\")\n",
    "sdf1.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_3 = sdf1.repartition(3)\n",
    "sdf_3.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+----------+\n",
      "|partition_id|count|count_perc|\n",
      "+------------+-----+----------+\n",
      "|           0| 6667|    33.335|\n",
      "|           1| 6667|    33.335|\n",
      "|           2| 6666|     33.33|\n",
      "+------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows_per_partition(sdf_3, num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_12 = sdf1.repartition(12)\n",
    "sdf_12.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+----------+\n",
      "|partition_id|count|count_perc|\n",
      "+------------+-----+----------+\n",
      "|           0| 1667|     8.335|\n",
      "|           1| 1666|      8.33|\n",
      "|           2| 1666|      8.33|\n",
      "|           3| 1666|      8.33|\n",
      "|           4| 1667|     8.335|\n",
      "|           5| 1667|     8.335|\n",
      "|           6| 1667|     8.335|\n",
      "|           7| 1667|     8.335|\n",
      "|           8| 1666|      8.33|\n",
      "|           9| 1667|     8.335|\n",
      "|          10| 1667|     8.335|\n",
      "|          11| 1667|     8.335|\n",
      "+------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows_per_partition(sdf_12, num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.conf.set(\"spark.sql.shuffle.partitions\", 200)\n",
    "sdf_col_200 = sdf1.repartition(\"idfirst\")\n",
    "sdf_col_200.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+----------+\n",
      "|partition_id|count|count_perc|\n",
      "+------------+-----+----------+\n",
      "|           3| 1111|     5.555|\n",
      "|          18| 1111|     5.555|\n",
      "|          26| 1111|     5.555|\n",
      "|          35|    1|     0.005|\n",
      "|          49| 1111|     5.555|\n",
      "|          75| 1111|     5.555|\n",
      "|         139| 1111|     5.555|\n",
      "|         144|11111|    55.555|\n",
      "|         166| 1111|     5.555|\n",
      "|         189| 1111|     5.555|\n",
      "+------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows_per_partition(sdf_col_200, num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-----+----------+\n",
      "|partition_id|idfirst|count|count_perc|\n",
      "+------------+-------+-----+----------+\n",
      "|           3|      7| 1111|     5.555|\n",
      "|          18|      3| 1111|     5.555|\n",
      "|          26|      8| 1111|     5.555|\n",
      "|          35|      0|    1|     0.005|\n",
      "|          49|      5| 1111|     5.555|\n",
      "|          75|      6| 1111|     5.555|\n",
      "|         139|      9| 1111|     5.555|\n",
      "|         144|      1|11111|    55.555|\n",
      "|         166|      4| 1111|     5.555|\n",
      "|         189|      2| 1111|     5.555|\n",
      "+------------+-------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows_per_partition_col(sdf_col_200, num_rows, \"idfirst\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 20)\n",
    "sdf_col_20 = sdf1.repartition(\"idfirst\")\n",
    "sdf_col_20.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+----------+\n",
      "|partition_id|count|count_perc|\n",
      "+------------+-----+----------+\n",
      "|           3| 1111|     5.555|\n",
      "|           4|11111|    55.555|\n",
      "|           6| 2222|     11.11|\n",
      "|           9| 2222|     11.11|\n",
      "|          15| 1112|      5.56|\n",
      "|          18| 1111|     5.555|\n",
      "|          19| 1111|     5.555|\n",
      "+------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows_per_partition(sdf_col_20, num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-----+----------+\n",
      "|partition_id|idfirst|count|count_perc|\n",
      "+------------+-------+-----+----------+\n",
      "|           3|      7| 1111|     5.555|\n",
      "|           4|      1|11111|    55.555|\n",
      "|           6|      4| 1111|     5.555|\n",
      "|           6|      8| 1111|     5.555|\n",
      "|           9|      2| 1111|     5.555|\n",
      "|           9|      5| 1111|     5.555|\n",
      "|          15|      0|    1|     0.005|\n",
      "|          15|      6| 1111|     5.555|\n",
      "|          18|      3| 1111|     5.555|\n",
      "|          19|      9| 1111|     5.555|\n",
      "+------------+-------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows_per_partition_col(sdf_col_20, num_rows, \"idfirst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_col_10 = sdf1.repartition(10, \"idfirst\")\n",
    "sdf_col_10.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+----------+\n",
      "|partition_id|count|count_perc|\n",
      "+------------+-----+----------+\n",
      "|           3| 1111|     5.555|\n",
      "|           4|11111|    55.555|\n",
      "|           5| 1112|      5.56|\n",
      "|           6| 2222|     11.11|\n",
      "|           8| 1111|     5.555|\n",
      "|           9| 3333|    16.665|\n",
      "+------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows_per_partition(sdf_col_10, num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-----+----------+\n",
      "|partition_id|idfirst|count|count_perc|\n",
      "+------------+-------+-----+----------+\n",
      "|           3|      7| 1111|     5.555|\n",
      "|           4|      1|11111|    55.555|\n",
      "|           5|      0|    1|     0.005|\n",
      "|           5|      6| 1111|     5.555|\n",
      "|           6|      4| 1111|     5.555|\n",
      "|           6|      8| 1111|     5.555|\n",
      "|           8|      3| 1111|     5.555|\n",
      "|           9|      2| 1111|     5.555|\n",
      "|           9|      5| 1111|     5.555|\n",
      "|           9|      9| 1111|     5.555|\n",
      "+------------+-------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows_per_partition_col(sdf_col_10, num_rows, \"idfirst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_col_5 = sdf1.repartition(5, \"idfirst\")\n",
    "sdf_col_5.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+----------+\n",
      "|partition_id|count|count_perc|\n",
      "+------------+-----+----------+\n",
      "|           0| 1112|      5.56|\n",
      "|           1| 2222|     11.11|\n",
      "|           3| 2222|     11.11|\n",
      "|           4|14444|     72.22|\n",
      "+------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows_per_partition(sdf_col_5, num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-----+----------+\n",
      "|partition_id|idfirst|count|count_perc|\n",
      "+------------+-------+-----+----------+\n",
      "|           0|      0|    1|     0.005|\n",
      "|           0|      6| 1111|     5.555|\n",
      "|           1|      4| 1111|     5.555|\n",
      "|           1|      8| 1111|     5.555|\n",
      "|           3|      3| 1111|     5.555|\n",
      "|           3|      7| 1111|     5.555|\n",
      "|           4|      1|11111|    55.555|\n",
      "|           4|      2| 1111|     5.555|\n",
      "|           4|      5| 1111|     5.555|\n",
      "|           4|      9| 1111|     5.555|\n",
      "+------------+-------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows_per_partition_col(sdf_col_5, num_rows, \"idfirst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Repartition from 4 to 3\")\n",
    "sdf_3.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Repartition from 4 to 12\")\n",
    "sdf_12.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Repartition from 4 to 5 with col\")\n",
    "sdf_col_5.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "sc.setJobDescription(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
