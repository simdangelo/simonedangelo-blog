---
date: 2024-08-13
modified: 2024-09-04T22:06:11+02:00
---
# 0. Resources
* [F2023 \#04 - Database Storage Part 2 (CMU Intro to Database Systems)](https://www.youtube.com/watch?v=UuHw3SQMrog&list=PLSE8ODhjZXjbj8BMuIrRcacnQh20hmY9g&index=6)
--- 

In the last lesson we discussed alternatives to tuple-oriented storage scheme:
* Log-structured storage: instead of storing the actual tuples, you store the log entries of the changes you've made to tuples.
* Index-organized storage

These three approaches we talked about (the Tuple-Oriented slotted pages, the Log-structured storage, and the Index-organized storage), are ideal for **write-heavy** (`INSERT`/`UPDATE`/`DELETE`) workloads. But the most important query for an application may be **read performance** (`SELECT`). And these approaches may not be the best way to go.
# 1. Database Workloads
Let's see some broad categories or database applications and some motivation for why we want to look at an alternative storage scheme:
* **On-Line Transaction Processing** (**OLTP**)
* **On-Line Analytical Processing** (**OLAP**)
* **Hybrid Transaction + Analytical Processing**
## 1.1. On-Line Transaction Processing (OLTP)
An OLTP workload is characterized by fast, short running operations, repetitive operations and simple queries that operate on single entity at a time. OLTP workloads typically handle more writes than reads, and only read/update a small amount of data each time.

**Example: Amazon storefront**. When you go to the Amazon website you look at products, then you click things, you add them to your card and then you purchase them and maybe you go under your account information and you go update your email address, or payment information and so on. Those operation are all considered **OLTP style workload** because you're making changes to small subset of the database.

**Example: posting things on Reddit**. It makes small changes to database (potentially huge database), but the amount of change each query is making is small and the amount of data is reading is small.
## 1.2. On-Line Analytical Processing (OLAP)
An OLAP workload is characterized by long running, complex queries and reads on large portions of the database. In OLAP workloads, the database system is often analyzing and deriving new data from existing data collected on the OLTP side. It's about running queries that extract new information across the entire dataset. OLAP operations are primarily read-heavy or read-only, so I'm not doing single updates.

**Example: retrieving stats in Amazon**. Run a query to find the number one sold product in the state of Pennsylvania on Saturday when the temperature is above 80 degrees. So, it's not looking a single person or looking at a single entity, but it's looking across the entire table, potentially doing a lot of joins.
## 1.3. Hybrid Transaction + Analytical Processing
A new type of workload (which has become popular recently) is HTAP, where OLTP and OLAP workloads are present together on the same database.
## 1.4. Visualizing Workload Types
Use a simple grid where along the x-axis we're saying wether the workload is made read-heavy vs write-heavy and on the y-axis we're saying how complex the queries are:
![](Pasted%20image%2020240814110204.png)

We'll talk about why the **things we talked about so far they're gonna be good for OLTP, but not for OLAP**, and then we'll design a **storage scheme that is better for OLAP**.
## 1.5. Example: Wikipedia Database
Let's make an example using a real database that is roughly what the Wikipedia database looks like:
![](Pasted%20image%2020240814111656.png)

## 1.6. Observations on Storage Models
The **Relational Model does not specify anything about how we should store the data in a table**: it does not specify that the DBMS must store all a tuple's attribute together in a single page. This solution may not actually be the best layout for some workloads, like OLAP workloads.
### 1.6.1. OLTP queries
**On-line Transaction processing**: simple queries that read/update a small amount of data that is related to a single entity in the database. An example:
```sql
-- example 1
SELECT P.*, R.*
FROM pages AS P
INNER JOIN revisions AS R
ON P.latest = R.revID
WHERE P.pageID = ?;

-- example 2
UPDATE useracct
SET lastLogin = NOW(),
	hostname = ?
WHERE userID = ?;
```
This kind of operations are what usually people end up with when they build a new application, like you create a startup and you start building some online service, you usually end up with something like above because you have no data in the beginning.
### 1.6.2. OLAP queries
On-line Analytical Processing: complex queries that read large portions of the database spanning multiple entities. An example to get the number of times people have logged in per month if they end with a ".gov" hostname):
```sql
SELECT COUNT(U.lastLogin),
		EXTRACT(month FROM U.lastLogin) AS month
FROM useracct AS U
WHERE U.hostname LIKE '%.gov'
GROUP BY
EXTRACT(month FROM. U.lastLogin)
```
You execute these workloads on the data you have collected from your OLTP application(s).
# 2. Storage Models
A DBMS's **Storage Model** specifies how it physically organizes tuples on disk and in memory.

Until now, we assumed that all the attributes are contiguous for a tuple and that's sort of roughly called a row-store, but for OLAP it may not be the best thing.

The reason why we have to discuss this part of as system is because there is a clear distinction in the database marketplace now between a row-store system and a column-store system:
* row-store systems are used for OLTP
* column-store systems are used for OLAP

When someone tries to sell you a row-store system that you can use for OLAP workload, you have to be very skeptical.

There are different ways to store tuples in pages. We have assumed the n-ary storage model so far:
1. N-ary Storage Model (NSM) (row-store)
2. Decomposition Storage Model (DSM) (column-store)
3. Hybrid Storage Model (PAX) (the most common for column-store)
## 2.1. N-ary Storage Model (NSM)
In the n-ary storage model, the DBMS stores all of the attributes for a single tuple contiguously in a single page (this is the assumption we're keeping from the beginning of this course).

This approach is ideal for OLTP workloads where requests are insert-heavy and transactions tend to operate only an individual entity. It is ideal because it takes only one fetch to be able to get all of the attributes for a single tuple.
### 2.1.1 NSM: Physical Organization
A disk-oriented NSM system stores a tuple's fixed-length and variable-length attributes contiguously in a single slotted page.

The following is basically the same layout we have seen before where we have a database page, an header in the front, the slot array, and then, as we start scanning the table and we want to inserting data, it's just going to go append the entries to the end and keep adding more and more (*not sure about this last sentence*). The tuple's **record is** (page#, slot#) i how the DBMS uniquely identifies a physical tuple. Let's look visually at the first tuple:
![](Pasted%20image%2020240814142948.png)
![](Pasted%20image%2020240814142743.png)

Then let's see all other tuples until the last one:
![](Pasted%20image%2020240814143052.png)

Let's see how this works in the Wikipedia example where we have someone wants to login with username and password:
```sql
SELECT * FROM useracct
WHERE userName = ?
AND userPass = ?
```

Without going into the details about how to retrieve the record ID given `userName` and `userPass`, let's just to the point where we go in the page directory and we find the page that has the data we are looking for, we look in the slot array, we jump to some offset and now we have all the data we need:
![](Pasted%20image%2020240814145004.png)

Again, this is ideal for OLTP because all the data is contiguously.

If we want to perform an insert all we need to do is to look in our page directory, find a page that has a free slot, go bring it to memory, and append to the end:
![](Pasted%20image%2020240814145732.png)

Let's try to run the complex query we have seen before representing an OLAP workload and let's see why the NSM is not suitable for it:
```sql
SELECT COUNT(U.lastLogin),
		EXTRACT(month FROM U.lastLogin) AS month
FROM useracct AS U
WHERE U.hostname LIKE '%.gov'
GROUP BY
EXTRACT(month FROM. U.lastLogin)
```
I need to scan all the pages in the table (because I need to look at all the user accounts), then I bring a page in and execute the query. The problem is that in this query only `lastLogin` and `hostname` attributes are relevant:
![](Pasted%20image%2020240814153950.png)
So, the obvious problem is that we brought a bunch of **data we actually don't need** (useless data, useless I/O)

### 2.1.2 NSM: Summary
+ **Advantages**
	+ fast insert, updates, and deletes
	+ good for queries that need the entire tuple (OLTP)
	+ can use index-oriented physical storage for clustering
+ **Disadvantages**
	+ not good for scanning large portions of the table and/or a subset of the attributes
	+ terrible memory locality in access patterns
	+ not idea for compression because of multiple value domains within a single page.
## 2.2. Decomposition Storage Model (DSM)
In the **Decomposition Storage Model**, the DBMS stores a single attribute (column) for all tuples contiguously in a block of data. Thus, it is also known as a “**column store**.” This model is ideal for OLAP workloads with many read-only queries that perform large scans over a subset of the table’s attributes that I really need.

Note again the benefit of a declerative language like SQL, where you don't care wether you're running on a row storage system or a column storage system. You write a query and it's the DBMS's responsibility to produce results in the best possible way.
### 2.2.1. DSM: Physical Organization
It maintains a separate file per attribute with a dedicated header area for metadata about entire column:
![](Pasted%20image%2020240814161415.png)

The DBMS stores the values of a single attribute across multiple tuples contiguously in a page. Also known as a "column store". Let's take again the Wikipedia example where we take every column of the table and we store that as a separate page:
![](Pasted%20image%2020240814170628.png)

Let's analyze the complex query again, but now with DSM, in particular let's focus first on the `hostname` attribute:
![](Pasted%20image%2020240814171335.png)
then let's focus on `lastLogin`:
![](Pasted%20image%2020240814181049.png)

Compared to NSM, now I have a 100% complete utilization of all the data that I brought in because I'm only bringing in the data I need for the query, and I'm not bringing in useless data.

We'll talk later on about query execution, but here's just an hint: from the `WHERE` clause, assume you keep track of a list of the offsets of the tuples within the `hostname` column that matches the condition in the query; then take those offsets and use them to fetch data for `lastLogin` attribute. How does this step happen? There's 2 approaches:
1. **Fixed-length Offsets**: each values is the same length for an attribute. The value in a given column will belong to the same tuple as the value in another column at the same offset. Therefore, every single value within the column will have to be the same length. Indeed this assumption is broken by varchar datatype.
2. **Embedded Tuple Ids**: each value is stored with its tuple id in a column. For every attribute in the columns, the DBMS stores a tuple id (ex: a primary key) with it. Then, the system would also store a mapping to tell it how to jump to every attribute that has that id. Note that this method has a large storage overhead because it needs to store a tuple id for every attribute entry.
![](Pasted%20image%2020240814210827.png)

*(These two approaches are not so clear to me. revise them asap)*

Since Fixed-length Offsets approach is more common, the problem we have to deal is how to convert variable-length values into fixed-length values. Solution: use **dictionary compression** to convert repetitive variable-length data into fixed-length values (typically 32-bit integers).

DSM System History:
* 1970s: Cantor DBMS
* 1980s: DSM Proposal
* 1990s: SybaseIQ (in-memory only)
* 2000s: Vertica, Vectorwise, MonetDB
### 2.2.2. DSM: Summary
* **Advantages**
	* reduces the amount wasted I/O per query because the DBMS only reads the data that it needs.
	* Faster query processing because of increased locality and cached data reuse. That's because we're literally going through columns one after another and not have to jump around within memory, which is better for CPUs.
	* Better data compression (more on this later).
* **Disadvantages**
	* slow for point queries, inserts, updates, and deletes because of tuple splitting/slitching/reorganization

### 2.2.3. Observation
OLAP queries almost never access a single column in a table by itself (in the previous query we queries against the `hostname` column, it's very common to query against many columns at the same time). At some point during query execution, the DBMS must get other columns and stich the original tuple back together. So, we need a way to have the attributes that are going to be used together somewhat closer to each other on disk or files, but still get all the benefit of a **column store** layout. That's what the PAX storage model is.
## 2.3. Hybrid Storage Model (PAX)
In the hybrid Partition Attributes Across storage model, the DBMS vertically partitions attributes within a database page (this is what Parquet and Orc use). The goal of doing so is to get the benefit of **faster processing** on columnar storage while retaining the **spatial locality** benefits of row storage.
### 2.3.1. PAX: Physical Organization
In PAX the rows are horizontally partitioned into groups of rows. Within each row group, the attributes are vertically partitioned into columns. Each row group is similar to a column store for its subset of the rows:
![](Pasted%20image%2020240814220101.png)
A PAX file has a global header containing a directory with offsets to the file’s row groups, and each row group maintains its own header with meta-data about its contents. Now, if I have a `WHERE` clause on both `Col A` and `Col B`, when I go fetch these pages for this row group, I have all the data for that I need close to each other. On the same time, I'm getting the benefit of I/O because this row group is going to be in tens of MB instead of 4 or 8 KB pages.

This is roughly how **Parquet** works.
# 3. Database Compression
Disk I/O is (almost) always the main bottleneck I/O. To speed up queries, you can skip data (that's what column-store systems do), but that's not the only way to do it. Another way is **Compression**: the DBMS can **compress** pages to increase the utility of the data moved per I/O operation (indeed it's widely used in disk-based DBMSs). The DBMS can fetch more useful tuples if they have been compressed beforehand at the cost of greater computational overhead for compression and decompression.

There is a trade-off that is **speed vs compression ratio**:
+ compressing the database reduces DRAM requirements
+ compressing may decrease CPU costs during query execution.

*(This introduction to Compression topic is not so clear. revise them asap)*

**Database Compression goals:**
1. must produce fixed-length values; this is because the DBMS should follow word-alignment and be able to access data using offsets.
2. postpone decompression for as long as possible during query execution (aka **late materialization**);
3. must be a **lossless** scheme.

**Lossless vs Lossy compression**
* when a DBMS uses compression, it is always **lossless** because people don't like losing data;
* any kind of **lossy** compression must be performed at the application level.
## 3.1. Compression Granularity
The kind of data we want to compress greatly affects which compression schemes can be used. There are four levels of compression granularity:
1. **Block-level**: compress a block of tuples for the same table.
2. **Tuple-level**: compress the contents of the entire tuple (NSM-only).
3. **Attribute-level**: compress a single attribute within one tuple (overflow); can target multiple attributes for the same tuple.
4. **Column-level**: compress multiple values for one or more attributes stored for multiple tuples (DSM-only).
## 3.2. Block-level Compression (Naive Compression)
"Naive" means that the DBMS is making a call to a third-part library (such as gzip) that's going to take the page and then compress it down to some binary form where the DBMS has no way to interpret or any can do any introspection into the compressed version of the block.

Some examples of general purpose algorithms: gzip, LZO, LZ4, Snappy, Brotli, Oracle OZIP, Zstd (from Facebook, it's considered the state-of-the-art compression scheme now).

An example of using naive compression is in **MySQL InnoDB**. The DBMS compresses disk pages, pads them to a power of 2KBs and stores them into the buffer pool. However, every time the DBMS tries to read/modify data, the compressed data in the buffer pool must first be decompressed:
![](Pasted%20image%2020240815172256.png)

If I'm doing a blind write (such as insert, or delete, or even update), I don't need to decompress the page I just write that change to the *mod log*. In some cases I can read on the mod log because if the data I need was just inserted and it's the mod log, I don't have to decompress the rest of the page.

*(This **MySQL InnoDB** part is not so clear. revise them asap)*

**Observations**
Ideally, we want the DBMS to operate on compressed data without decompressing it first. The easiest way to do is to go into a **Column-level** decompression type.
## 3.3. Column-level Compression
A bunch of compression algorithm in this category:
+ **Run-length Encoding**
+ **Bit-Packing Encoding**
+ **Bitmap Encoding**
+ **Delta Encoding**
+ **Incremental Encoding**
+ **Dictionary Encoding** (default choice for most systems). After you do dictionary encoding, you can apply all these other compression schemes on the dictionary itself or you're still your dictionary code of value and get even further compression. So you can sort of a multiplicative effect where you do compression one way and then you run another compression algorithm on the compressed data and get even better compression. And it's done in a way where the data system can natively interpret what those bytes actually means in the compressed form without having to decompress it first. And this is why you want the data system to do everything do everything and don't want the OS to do anything or anybody else do anything because we can do this native compression.
### 3.3.1. Run-length Encoding
The idea is: if you have contiguous runs of the same value, instead of throwing that value over and over again for every single tuple, all instead store a compressed summary that says: "for this value at this offset, here's how many occurrences it has". This works great if your data is sorted on whatever the column you're trying to compress and you can't always do that.

Let's say we have a sample table with `id` and `isDead` columns. We can compress the `isDead` column:
![](Pasted%20image%2020240831201354.png)

The compression is made by scanning the `idDead` column and finding the contiguous or continuous tuples that have the same value and then converting it into the so called RLE Triplet in the following form: `(Value, Offset, Length)`. Now, if the following query comes along
```sql
SELECT idDead, COUNT(*)
FROM users
GROUP BY isDead
```
the system can just read through the `isDead` column and compute the aggregation in the query by summing up the length of the run and then along with the value.

If we sort the data, the RLE compression has only two entries in the compressed column:
![](Pasted%20image%2020240831201410.png)
### 3.3.2. Bit-Packing Encoding
People oftentimes declare attributes or columns of a certain types that is larger than they actually need. So, if I have a column declared as integer type (that's in SQL is 32-bit integer), no matter if this column contains small values because I'm still going to allocate 32-bit to store them:
![](Pasted%20image%2020240901124405.png)

In this example, we need 236 bits to store these 8 values. However, the actual data that I really need is contained in the red box, meaning I need only 8 bits for each value and no longer 32. Even though I declared it as 32-bit integer, I'm going to store values as 8-bit integer:
![](Pasted%20image%2020240901125214.png)

This method reduces the size down by a factor of 4.

However, what happens if I have a number that can't be stored in those 8 bits that I'm trying to pack them into? We'll see that with the next compression technique.
### 3.3.3. Mostly Encoding
This is a variation of bit packing. The idea here is to consider that most of the data in my column are small enough and we'll use a special marker to indicate when a value exceeds the largest size and for them we'll store them in a look-up table:
![](Pasted%20image%2020240901125844.png)
### 3.3.4. Bitmap Encoding
If you have an attribute with low cardinality (meaning it has a small number of unique values), instead of storing for every single tuple in a column the actual value, I want to store a separate bitmap for each unique value in the column and the bit is set to 1 based on whether the tuple at that offset has that particular value. Here's a visual explaination:
![](Pasted%20image%2020240901145743.png)

In this case we have still two possible values and we have two bitmaps (one for `Y` and one for `N`). So we need 2x8-bits for storing `Y` and `N` and the bitmap needs 18 bits (because I have 9 values and I need 2 bits each); so this method reduces the size down from 72 bits to 34 bits:
![](Pasted%20image%2020240902215808.png)

**Problem**: if data is high-cardinality, this method would be a terrible idea. Assume you have 10 million tuples and suppose you have `zip_code` column in a table (there are 43 thousand zip codes in US). Assuming we store the zip code with 32 bits:
+ the raw data is 10.000.000 x 32 bits = 40 MB;
+ but if I had to have a 10 million size bitmap for every single zip code, we need 10.000.000 x 43.000 = 53.75 GB.

Furthermore every time the application inserts a new tuple, the DBMS must extend 43.000 
different bitmaps.
### 3.3.5. Delta Encoding
The idea is: if the values from one tuple to the next are enough close to each other, I don't need to store the entire value for one tuple, but I just need to store the difference between the previous value. For example, if you want to record the temperature of a room every minute, it is likely to vary slightly:
![](Pasted%20image%2020240902231229.png)

I can compress even further with RLE, getting finally a combination of Delta Encoding and RLE:
![](Pasted%20image%2020240903214608.png)

This method reduces the size of `time64` column down from 320 bits to 96 bits (if you think of billions of records, this would be a massive savings).
### 3.3.6. Dictionary Compression
This is the most common compression technique. The idea is: if we have values that occur frequently, instead of storing that value repeatedly within a column, we're going to convert that into some 32-bit integer and then we maintain a mapping data structure (the dictionary) that knows how to take that 32-bit integer and convert it back into the original value. Typically it's a one-on-one correspondence where one value corresponds to one dictionary code. Of course we need a way to do fast encoding-decoding process on the fly that allows to do both range and point queries. Look at this example:
![](Pasted%20image%2020240903222240.png)

In this example, the string `Andy` can be converted into a corresponding dictionary code by first performing a lookup in the dictionary. Once the string is encoded, I can scan through the column and make comparisons directly based on the integer codes rather than the original strings.

A dictionary needs to support two operations:
+ **Encode/Locate**: for a given uncompressed value, convert it into is compressed form.
+ **Decode/Extract**: for a given compressed value, convert it back into its original form.

No magic hash function will do this for us because they would generate something much larger than the original value, so they don't get it down to 32-bit integer. So, we're going to build a data structure that we maintain that allows to do this.

Furthermore we want something that **preserved the ordering of the original values** such that the compressed dictionary code will have the same ordering as the original data. If the order is preserved, this allows to do queries like the one:
![](Pasted%20image%2020240903232652.png)

 In this example the `LIKE` clause is converted into a `BETWEEN` clause because by following this steps:
 + look up in the Sorted Dictionary;
 + run the `LIKE` clause just on the dictionary `value` and find the `code` that match;
 + find the minimum and the maximum values for the matching values (`10` and `20` in our example);
 + now you have the values to put in the `BETWEEN` clause.

Let's take a look at the following query:
```sql
SELECT name FROM users
WHERE name LIKE 'And%'
```
Still must perform scan on column (*Honestly, I don't understand what that means. Maybe that compression doesn't work? Or it means that I need to scan the entire compressed column? Maybe this latter hypothesis is right because the following example shows how it's possibile to scan only the dictionary instead of the entire column*)

Let's take a look at the following query:
```sql
SELECT DISTINCT name FROM users
WHERE name LIKE 'And%'
```
In cases like this one, the system can even be smarter and can answer queries without actually looking at the compressed data, but just **operate directly on the dictionary**. In this case I only just need to know what values actually exist in the dictionary and I don't need to go look at the actual column. If you assume you have only 4 unique `name` values in a table with billion of rows, I only need to look at 4 rows in the dictionary to answer to this query.

>[!note] Parquet and ORC limitations
>Parquet and ORC don't actually expose the dictionary to you when you use their libraries and utilities, so you can't do this trick. Parquet and ORC decompress the data when it gives back to you, you can't operate directly on compressed data. That's one of the biggest limitations of these two formats.

#### Dictionary: Data Structures
What is the data structure we're going to use for our dictionary?
+ **Array**: this works great if the files are immutable because i build the array once and I never have to resize and insert things in place to move things around. I can just build once and I'm done. This is the most common choice.
+ If you need something dynamic and can support updates use **Hash Table** and/or **B+Tree**.

Let's explore a little bit the **Array** structure:
 * First store the values and then store them sequentially in a byte array;
 * store the size of the value if they are variable-length;
 * replace the original data with the dictionary codes that are the (byte) offset into this array.

Graphically:

![](Pasted%20image%2020240904213248.png)
# 4. Conclusion
It is important to choose the right storage model for the target workload:
+ **OLTP = Row Store**
+ **OLAP = Column Store**

DBMSs can combine different approaches for even better compression. Dictionary encoding is probably the most useful scheme because it does not require pre-sorting.

The distinction between **Row Store** and **Column Store** systems have ramifications throughout all other parts of the data system. So, it's important to understand the trade-offs between these two approaches.
# 5. Next Lesson
[Some lectures ago we showed that there were two problems is Database Storage](03.%20Database%20Storage%20Part%201.md#4.%20Database%20Storage):
1. How the DBMS represents the database in files on disk. That's what we have covered so far.
2. How the DBMS manages its memory and moves data back-and-forth from disk.