---
date: 2024-09-21
modified: 2024-09-23T19:27:16+02:00
---

**Resources**:
* [# F2023 #07 - Hash Tables (CMU Intro to Database Systems](https://www.youtube.com/watch?v=eBgKVqFUUlA&t=4s&ab_channel=CMUDatabaseGroup)

---

Let's recall the course outline:
+ **Disk Manager** - from the first lesson on
+ **Buffer Pool Manager** - last lesson
+ **Access Methods** - from this lesson!
+ **Query Planning** - not yet
+ **Operator Execution** - not yet

Right now we know how pages are organized, how to put data in them, so actually now it's time to build the execution engine to interpret what those pages are storing and run queries and store data and do all things that you expect a database system to do for us. Specifically, in the next couple of weeks, we're going to focus on what we call **Access Methods**, and these are going to be the internal components of the system that's going to allow us to access data or access the pages we're storing and derive meaning from them.

We'll use two data structures:
+ **Hash Tables** (unordered)
+ **Trees** (ordered)
# 1. Data Structures
A DBMS uses various data structures for many different parts of the system internals. Some examples include:
* **Internal Meta-Data**: This is data that keeps track of information about the database and the system state. Ex: Page tables, page directories.
* **Core Data Storage**: Data structures are used as the base storage for tuples in the database.
* **Temporary Data Structures**: The DBMS can build ephemeral data structures on the fly while processing a query to speed up execution (e.g., hash tables for joins).
* **Table Indices**: Auxiliary data structures can be used to make it easier to find specific tuples.

There are two main **design decisions** to consider when you want to implement data structures for the DBMS:
1. **Data Organization**: how we layout data structure in memory/pages and what information to store to support efficient access. Of course we want to avoid NP complete problems. 
2. **Concurrency**: we need to think how to allow multiple threads or workers or processes in our DBMS to access the data structure at the same time without causing problems. We'll cover this in the next class, but basically there's two types of problems:
	+ **Physical Layout Problems** (or Physical Integrity Problems), where two threads are trying to write to the same thing at the same time and they collaborate each other,
	+ **Logical Problems**, where two threads try to insert the same key at the same time. What should be happen? Should I have duplicate keys, or should one of them fail, or what?

# 2. Hash Tables
A **Hash Table** implements what we call **unordered associative array** that maps keys to values. It uses a Hash Function to compute an offset into this array for a given key, from whoch the desired value can be found.

Because this has function is basically taking any arbitrary byte array, running through this has function and then you come out with this random number (so it's unordered), you're basically making this be completely random I/O. This **randomness** is useful because it will help handle skew. Ideally, your hash function will handle skew well, allowing the data to be evenly distributed throughout the data structure, which has many benefits.

Complexity of hash tables:
* **Space complexity**: O(n), where n that is the number of keys that we need to store
* **Time complexity**:
	* on average: O(1), meaning like in most cases I do my lookup into my hash tables and I find exactly the thing I'm looking for;
	* worst case scenario: O(n), meaning like I have to look at every single key to find the thing that I am looking for.

O(1) sound amazing, but in reality there's a constant factor involved in this and we actually going to care about them (i.e. if our hash function is super slow and it takes 100 milli seconds to compute the hash, then we jump to the hash table and it's O(1), but 100 milli seconds is a long time).

Here's a basic **Static Hash Table**:
![](Database%20System/attachments/Pasted%20image%2020240922161529.png)
The assumption here is that we know in advance the number of keys (say n) and we just have this giant array with every possible key where we have a pointer to some location on a page or in memory that has the data that we want. It technically works, but it's not a great idea. There are **three unrealistic assumption** (that are not alway realistic) that explain why this simple static has table is not going to work:
1. Number of elements is known ahead of time and fixed.
2. Each key is unique.
3. Perfect hash function guarantees no collisions because it guarantees that for any unique key you get a unique hash value: `key1≠key2`, then `hash(key1)≠hash(key2)`.

A hash table implementation is comprised of two parts:
1. **Design Decision 1: Hash Function**. This tells us how to map a large key space into a smaller domain and it's typically going to produce a fixed-length value, like a 32-bit integer or 64-bit integer. So when we decide which hash function to use there will be a trade-off between being fast vs. collisions rate (meaning that for two or more distinct keys, I get back the same hash value). For example, the fastest possible hash function is to reduce each key to 0 or 1 for example: in this case the collision rate is terrible, but it's going to be fast. Of course we need something in the middle between these two extremes.
2. **Design Decision 2: Hashing Scheme**. This tells us the protocol we're going to use to handle key collisions. Even in this case there is a tradeoff between allocating a very large hash table with a bunch of slots where we could put keys in (in this case, if it's very large, it's very unlike to have collisions, but we need to store a lot of memory to store this giant hash table) vs. having an hash table with 1 slot and everything collides into that and we need additional instructions to get/put keys. So, it's a tradeoff between storage and computation.
# 3. Hash function
* An **Hash Function** is a function so that, for any input key, returns an integer representation of that key. It converts arbitrary byte array into a fixed-length code.
* As we already said, we want something **fast** and with a low **collision rate**.
* We **don't want to use a cryptographic hash function** for DBMS hash tables (e.g. SHA-2) because we do not need to worry about protecting the contents of keys. These hash functions are primarily used internally by the DBMS.

There's a lot of hash functions and most modern systems are not going to write their own hash function (maybe Postgres has one for historical reason):
+ CRC-64 (1975)
+ MurmurHash (2008)
+ Google CityHash (2011)
+ **Facebook XXHash (2012)**: it's the **state-of-the-art**.
+ Google FarmHash (2014)
# 4. Static Hashing Schemes
A static hashing scheme is one where the size of the hash table is fixed. This means that if the DBMS runs out of storage space in the hash table, then it has to rebuild a larger hash table from scratch, which is very expensive. Typically the new hash table is twice the size of the original hash table.

Assuming we're running a specific hash function, we're going to talk how the hash table is going to look like and how do we handle collisions. There are two variants that are both categorized as what are called **Open Addressing Hash Tables**, meaning that the location of a key that we're putting in the hash table is open (meaning that it doesn't always have to be at the exact same location every single time):
1. **Linear Probe Hashing**
2. **Cuckoo Hashing**

There are several other schemes covered in the Advanced DB course: Robin Hood Hashing, Hopscotch Hashing, Swiss Tables.
## 4.1. Linear Probe Hashing
This is the most basic hashing scheme, but also the fastest. It's a giant array of slots that are fixed and if we wanted to insert we hash into it and, if the slot is free we insert the thing we're looking for, or if the slot it's not free we look at the next slot and insert in or we keep looking until we have a free slot. This is the way to avoid collisions.

Google’s *absl::flat_hash_map* is a state-of-the-art implementation of Linear Probe Hashing.

This is called open addressing hashing because the idea is that there's no guarantee that for a given key it's going to always be in the same address or same location in the slot.

Let's see how it works. We want to insert the `A` key, so we hash it and apply the mod by the number of slots we have and then that key will land on this location; so we insert the key along with the value together:
![](Database%20System/attachments/Pasted%20image%2020240922211944.png)
The reason why we need the keys is that if we go to a lookup for looking for `A`, we'll hash at the same location, but now we're going to do an equality check to see whether the key we are looking for is the key in a given slot.

Say we want insert `C`. We hash `C` and it will land at the same location of `A`, so that slot is occupied and we cannot insert there. So, we just follow down to the next slot and insert the key there:
![](Database%20System/attachments/Pasted%20image%2020240922212343.png)
The same for all other keys:
![](Database%20System/attachments/Pasted%20image%2020240922214828.png)
![](Database%20System/attachments/Pasted%20image%2020240922212416.png)

For **lookups**, we can check the slot the key hashes to, and search linearly until we find the desired entry. If we reach an empty slot or we iterated over every slot in the hashtable, then the key is not in the table. Note that this means we have to store both key and value in the slot so that we can check if an entry is the desired one. **Deletions** are more tricky. We have to be careful about just removing the entry from the slot, as this may prevent future lookups from finding entries that have been put below the now empty slot. Say we delete `C` and then we do a lookup on `D`:
![](Database%20System/attachments/Pasted%20image%2020240922215233.png)
In this case `D` is going to hash to that empty spot and, due to the way the protocol works, if I see an empty slot, I know I'm done. There are some solutions:
+ rehash keys until you find the first empty slot:
	![](Database%20System/attachments/Pasted%20image%2020240922215316.png)
	This is not a good idea because you have to move everything (billion of rehashes potentially).
+ **Tombstone**: after deleting `C`, put a marker to indicate that the entry in the slot is logically deleted. So, when I lookup `D`, it sees the tombstone and it looks down and keep scanning along until it finds thing it's looking for:
	![](Database%20System/attachments/Pasted%20image%2020240922215734.png)

	You can reuse the slot with the mark of tombstone for new keys because you just need to insert over top of it and that doesn't break the flow in the hash table:
	![](Database%20System/attachments/Pasted%20image%2020240923191623.png)
	Periodically it may need garbage collection because you can collect many tombstone marker and it's a waste of space.
### Non-Unique Keys
In the case where the same key may be associated with multiple different values or tuples, there are two approaches:
1. **Separate Linked List**: Instead of storing the values with the keys, we store a pointer to a separate storage area that contains a linked list of all the values, which may overflow to multiple pages:
	![](Database%20System/attachments/Pasted%20image%2020240923192636.png)
2. **Redundant Keys**: The more common approach is to simply store the same key multiple times in the table:
	![](Database%20System/attachments/Pasted%20image%2020240923192728.png)