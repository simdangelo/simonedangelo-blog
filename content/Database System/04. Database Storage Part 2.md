---
date: 2024-07-01
modified: 2024-09-04T22:05:56+02:00
---

# 0. Resources
* [F2023 \#04 - Database Storage Part 2 (CMU Intro to Database Systems)](https://www.youtube.com/watch?v=Ra50bFHkeM8&list=PLSE8ODhjZXjbj8BMuIrRcacnQh20hmY9g&index=5)

---

# 1. Log-Structured Storage
This is a kind of recap of the last lesson:

**Insert a new Tuple**
- Check page directory to find a page with a free slot.
- Retrieve the page from the disk (if not in memory).
- Check the slot array to find space in page that will fit.

**Update an existing tuple using its record id**
- Check page directory to find location of page.
- Retrieve the page from disk (if not in memory).
- Find offset in page using slot array.
- If new data fits, overwrite the existing data. Otherwise, mark existing tuple as deleted and insert new version in a different page.

Some **problems** associated with the Slotted-Page Design are:
- Fragmentation: Deletion of tuples can leave gaps in the pages, making them not fully utilized.
- Useless Disk I/O: Due to the block-oriented nature of non-volatile storage, the whole block needs to be fetched to update a tuple.  
- Random Disk I/O: The disk reader could have to jump to 20 different places to update 20 different tuples placed in 20 different pages, which can be very slow.  

What if we were working on a system which **only allows creation of new pages and no overwrites?** (*Examples: some object stores, HDFS.*)

Remember that there are three different approaches to how I could actually organize the data inside of the pages:
1. **Tuple-oriented Storage** (Previous lesson), where we’re only storing tuples and the exact values that those tuples have.
2. **Log-structured Storage** (Today), where we store deltas of what changes since the last time time tuple was updated.
3. **Index-organized Storage** (Today), where we use a tree structure where in the leaf node we store the data itself.

The log-structured storage model works with the assumption above (*****) and addresses some of the problems listed above.  
  
**Log-Structured Storage**: Instead of storing tuples in pages, the DBMS only stores the log records of changes of these tuples. As the application makes changes to the database, the DBMS appends new log records to an in-memory buffer without checking previous records.
- Records contain the tuple’s unique identifier (you can’t use row id we did before because we don’t have pages, offsets, and slots), the type of operation (`PUT`/`DELETE` [we have no `INSERT` nor `UPDATE` because they’re just a `PUT`]), and, for `PUT`, the contents of the tuple.
- As the application insert data or makes changes, we’re going to append new log entries into an **in-memory page buffer** in the order they arrive **without checking previous log records** and then, when that buffer gets full, it’ll write out the disk. This is different than the tuple-oriented architecture, where we have to fetch the page that has the original tuple and then I can update.

![](Database%20System/attachments/3bf2e43fcbac1d07ab114e61e493744f.png)

Two things to point out:
1. In the page-oriented architecture I would have have N tuples split across N different pages, while with log-oriented architecture those N tuples are always on the same page when I wrote them out when the in-memory page gets full.
2. In log-oriented architecture once a page is written to disk, it’s immutable. We can never go back and do in-place updates.

Fast writes, potentially slow reads.
- Compared to tuple-architecture, **writing** in log-architecture is much faster because we’re just appending log records and write them out sequentially. Disk writes are sequential and existing pages are immutable  
    which leads to reduced random disk I/O. (  
    _this last sentence comes from the Notes on the course site, but i don’t understand what random disk I/O are about_)
- On the other hand **read** could be slower. To find the log record for a given key we first want to scan sequentially the in-memory-page in reverse order from newest to oldest to find the most recent log record that we want. If that log-record has had no write operation recently and it’s not in the in-memory page, we may have go to disk. This is not efficient.

**Slow Reading Solution**  
To avoid long reads, the DBMS can have  
**indexes for every single record id** indicating where in the in-memory buffer page are located or, if they’re not in memory, where they’re on disk. So, if you want to get the record id 104, I need to do some lookup in this index and it’ll tell you what offset in the memory buffer has the data I’m looking for and then I can jump to that specific location:

![](Database%20System/attachments/2495e09e0f09e1a88a8abb3fd29af124.png)

It’s like the glossary page at the end of a textbook: you look at the keyword you want, it tells the page number, and you can jump to that page without scanning through the page one after another.

**Log-Structured Compaction**
- The log will eventually get quite big. The DBMS can periodically compact the log by taking only the  
    most recent change for each tuple across several pages.  
    
    ![](Database%20System/attachments/0b5597489b432c1743ab709e3f9dcd1f.png)
    
- After compaction, the ordering is no longer needed since there’s only one of each tuple, so the DBMS  
    can sort by key value for faster lookup (  
    _I don’t understand why ordering by key value improves performance_):
    
    ![](Database%20System/attachments/26d1ed9ddeb16d357e78a88ff1829574.png)
    
    These are called **Sorted String Tables** (**SSTables**).
    

There are two main ways to make Compaction (the terminlogy we’ll use comes from RocksDB):
- In **Universal Compaction**, any log files can be compacted together:
    
    ![](Database%20System/attachments/5ab10e58b2aadb200decd42ea60dd077.png)
    
- In **Level Compaction**, the smallest files are level 0. Level 0 files can be compacted to create a bigger level 1 file, level 1 files can be compacted to a level 2 file, etc.:
    
    ![](Database%20System/attachments/3c19eb7f02205eb3f18997f130470c51.png)
    
    _(The difference between the twos is not clear to me)_
    

Log-structured storage managers are more common today. This is partly due to the proliferation of RocksDB.
- The downside is that compaction is expensive and also leads to write amplification (for each logical write, there could be potentially multiple physical writes).  

_(This part about compaction needs to be revised. it’s not so clear)_
# 2. Index-Organized Storage
Observe that both **tuple-oriented storage** and **log-structured storage** approaches rely on additional index to find individual tuples because the tables are inherently unsorted, so to find a specific tuple we need that additional index (i.e. in the tuple-oriented storage to get the record ID we used a mix of page number and slot number).

In the **Index-Organized Storage** scheme, the DBMS directly stores a table’s tuples as the value of an index data structure. The DBMS would use a page layout that looks like a slotted page, and tuples are typically sorted in page based on key.

_(This part about compaction needs to be revised. it’s not so clear)_

---

# 3. Data Representation
The data in a tuple is essentially just byte arrays and it’s the work of the DBMS, based on the schema that it’s stored in its catalog (when you use `CREATE TABLE`), to interpret what those bytes actually are.

A **Data Representation** scheme is how a DBMS stores the bytes for a value.

Taking into consideration this SQL code:
```sql
CREATE TABLE AndySux (
	id INT PRIMARY KEY,
	value BINGIT
);
```

think of it as a char array with:
- an header, that keeps track of the size of it, the nulls, etc.;
- after the header’s done, at first offset you would have the first column (`id` column in our case);
- the column id is an integer, so it’s going to be 32 bits; while the value column is a bigint, so it’s going to be 64bits.

Graphical representation:

![](Database%20System/attachments/e23d3e3dfd8c8bcc14214d67c9c2660a.png)

Internally, database system is find the start location of the tuple (using the slot array method, or whatever), then we jump to that offset in a page, then, considering that the header is always of the same size for every single tuple, we have to jump past that, and then we just do simple arithmetic to find the offset of the column we’re looking for based on the bits of each column.

One of things we have to be careful as we start storing these bits is dealing with the **alignment** to make sure that the data we’re storing aligned to how the CPU actually wants to operate on data. So basically DBMSs want to make sure the tuples are word-aligned to enable the CPU to access it without any unexpected  
behavior or additional work.  

Consider this table:
```sql
CREATE TABLE foo (
	id INT PRIMARY KEY,
	cdate TIMESTAMP,
	color CHAR(2),
	zipcode INT
);
```

Assume that we’re going to break up our char array representing this tuple into 64-bit words:

![](Database%20System/attachments/f62f02ddd24c1c1e2e8b98a8b37caa6e.png)

Then let’s store the columns:

![](Database%20System/attachments/6d5ab7a40591fedcbf825f2bb32efa49.png)

The problem is that the attribute `id`, for example, is going to span two words:

![](Database%20System/attachments/1355f213a0f2dc21c22fbac874b46dd2.png)

So, what happens in the CPU when you try to jump to a memory address to do some operation on something that spans the word boundaries? It depends:
- **Approach 1: Perform Extra Reads**. Execute two reads to load the appropriate parts of the data word and reassemble them. For example, x86 Intel makes your life easy by doing extra reads for us because they want to hide all the complexity of the architecture. However, this is going to make your data system run slower.
- **Approach 2: Random Reads**. Read some unexpected combination of bytes assembled into a 64-bit word. There’s no guarantee that the bits are going to land in the right order. Older CPUs would do that.
- **Approach 3: Reject**. Some architecture throw an exception and hope app handles it.

So we need to make sure that none of the attributes in our Tuple are going to span these boundaries. There are two approaches to handle this situation:

1. **Padding**: add empty bits after attributes to ensure that tuple is word aligned:
	![](Database%20System/attachments/image.png)
2. Reordering: switch the order of attributes in the physical layout to make sure they are aligned and pad if necessary:
	![](Database%20System/attachments/image%201.png)
	
	There are five high level datatypes that can be stored in tuples: integers, variable-precision numbers, fixedpoint precision numbers, variable length values, and dates/times.

Let’s see this word alignment phenomenon in practice in PostgreSQL. Firstly, PostgreSQL uses **Padding** and it has a function called `ROW` that takes a comma separated list and it makes a row:
```sql
SELECT row(1,2,3);
```

Let’s cast those values to, respectively, small int (2-byte int), regular int (4-byte int), and big int (8-byte int):
```sql
SELECT row(1::int2,2::int4,3::int8);
```

Let’s use PostgreSQL function `pg_column_size()` that tells the size of this tuple in bytes:
```sql
SELECT row(1::int2,2::int4,3::int8); -- it returns 40
SELECT row(1,2,3); -- it returns 36
```

Let’s make a more complex row mixing chars with integers:
```sql
SELECT row('a'::char, 2::int2, 'b'::char, 4::int4, 'c'::char, 8::int8)
```
and it returns `48`. However, if we re-do it by putting all the integers first we got `44`. That’s because Postgres has to pad things out to make sure that everything is 64-bit aligned, but it doesn’t do this for us automatically so you have to tell Postgres to do that. Some system can do this automatically.

There are five high level datatypes that can be stored in tuples:
1. **Integers**. Most DBMSs store integers using their “native” C/C++ types as specified by the IEEE-754 standard (this standard defines how hardware should represent datatypes). These values are fixed length. Examples: `INTEGER`, `BIGINT`, `SMALLINT`, `TINYINT`.
    
2. **Variable-precision numbers** (Examples: `FLOAT`, `REAL`). These are inexact, variable-precision numeric types that use the “native” C/C++ types specified by IEEE-754 standard. These values are also fixed length. Operations on variable-precision numbers are faster to compute than arbitrary precision numbers because the CPU can execute instructions on them directly. However, there may be rounding errors when performing computations due to the fact that some numbers cannot be represented precisely.
    
    Let’s show a scenario that looks ok:
    
    ![](Database%20System/attachments/image%202.png)
	
	However, if I increase the number of digits that I’m going to write out in the printf statement, it ends up with:
	
	![](Database%20System/attachments/image%203.png)
	Again, that’s because the hardware can’t represent `0.3` exactly and it’s going to be some approximation based on that. For this reason, database systems are also going to provide Fixed-point precision numbers.
3. **Fixed-point precision numbers** (Examples: `NUMERIC`, `DECIMAL`). These are numeric data types with arbitrary precision and scale. They are typically stored in exact, variable length binary representation (almost like a string) with additional meta-data that will tell the system things like the length of the data and where the decimal should be. These data types are used when rounding errors are unacceptable, but the DBMS pays a performance penalty to get this accuracy. As example, let’s see what Postgres does by directly looking at the source code:
	
	![](Database%20System/attachments/image%205.png)
	
	We can see that Postgres represents the type of a Numeric as some kind of _struct_ with a bunch of additional metadata about what the number actually is. But the core thing they’re storing internally along with this metadata is the `NumericDigit` array, that’s just a type cast to an unsigned char. So, they’re literally storing your decimal as a string value and then they use this metadata to figure out how to interpret that string to put it be the correct form.
4. **Variable length values**. These represent data types of arbitrary length. They are typically stored with a header that keeps track of the length of the string to make it easy to jump to the next value. It may also contain a checksum for the data. Most DBMSs do not allow a tuple to exceed the size of a single page. The ones that do store the data on a special “overflow” page and have the tuple contain a reference to that page. These overflow pages can contain pointers to additional overflow pages until all the data can be stored. Some systems will let you store these large values in an external file, and then the tuple will contain a pointer to that file. For example, if the database is storing photo information, the DBMS can store the photos in the external files rather than having them take up large amounts of space in the DBMS. One downside of this is that the DBMS cannot manipulate the contents of this file. Thus, there are no durability or transaction protections. Examples: `VARCHAR`, `VARBINARY`, `TEXT`, `BLOB`.
5. **Dates/Times**. Representations for date/time vary for different systems. Typically, these are represented as some unit time (micro/milli)seconds since the unix epoch. Examples: `TIME`, `DATE`, `TIMESTAMP`.

Because for integer types we’re relying on the hardware to store data, that typically means you can’t copy the raw database files that you generate from one architecture to another because the bits are going to be flip and it’ll get messed up. SQLite avoids this problem because it stores everything as `VARCHAR`, and at runtime they cast things base on the type in the attribute. In this way they can guarantee portability.
### Null Data Types
There are three common apporaches to represent nulls in a DBMS:
1. **Null Column Bitmap Header**: Store a bitmap in a centralized header that specifies what attributes are null. This is the most common approach. The size of this bitmap will vary based on the number of attributes you have, which we know whether it could be null or not because it’s in the `CREATE TABLE` statement.
2. **Special Values**: Designate a value to represent `NULL` for a data type (e.g., `INT32_MIN`).
3. **Per Attribute Null Flag**: Store a flag that marks that a value is null. This approach is not recommended because it is not memory-efficient. For each value, the DBMS has to use more than just a single bit to avoid messing up with word alignment.
### Large Values
Most DBMSs don’t allow a tuple to exceed the size of a single page (again: a page size is defined by the database system and every single page within that data or that table has to have the same page size). To store values that are larget than a page, the DBMS uses separate **overflow storage pages**:
- Postgres: TOAST (>2KB). So any attribute that’s larger than 2KB is stored in a separate page and in the actual tuple itself there’s a pointer and an offset that points to where to go find the actual value you’re looking for:
	
	![](Database%20System/attachments/image%206.png)
- MySQL: Overflow (>1/2 size of page)
- SQL Server: Overflow (>size of page)

### External Value Storage
Some systems allows you to store a large value in an external file in the local file system and internally store the URI, or the URL, or where the data is located, so that when you query against the table and you go get that attribute it goes to the OS and gets that data and copies it into its buffer and then hands it back to you:

![](Database%20System/attachments/image%207.png)

At the time of writing, perhaps only Oracle and Microsoft can do this:
- Oracle: `BFILE` data type
- Microsoft: `FILESTREAM` data type

The DBMS cannot manipulate the contents of an external file.